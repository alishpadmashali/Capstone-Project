{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a85b8865",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the required libraries:\n",
    "\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9db6184",
   "metadata": {},
   "source": [
    "# PHASE-I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1f346e",
   "metadata": {},
   "source": [
    "# 1.Skechers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b83a510",
   "metadata": {},
   "source": [
    "**First i will divide the Scrapping into two types, 1.Men and 2.Women**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5f3ccb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First i will scrap the details of men Shoes\n",
    "#Creating a empty list and at last we will append the scrapped values into it.\n",
    "#Scrapping of 150 shoes for men and Women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bebb8bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome()     #India\n",
    "driver.get('https://www.skechers.in/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc8978ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First will Scrap the Mens Shoes\n",
    "\n",
    "Men=driver.find_element(By.XPATH,'/html/body/div[1]/header/nav[2]/div/div[1]/div/div/div[1]/div/nav/div[2]/ul/li[2]')\n",
    "Men.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73fd40a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets scrape all the product url of all \"Mens\" Shoes:\n",
    "\n",
    "product_url_101=[]   \n",
    "\n",
    "start=0\n",
    "end=7\n",
    "\n",
    "for page in range(start,end):\n",
    "    url=driver.find_elements(By.XPATH,'//a[@class=\"link p-heading\"]')\n",
    "    for i in url:\n",
    "        product_url_101.append(i.get_attribute('href'))\n",
    "    next_button=driver.find_element(By.XPATH,'//a[@class=\"page-link next\"]')\n",
    "    next_button.click()\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7caf2c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_url_101=product_url_101[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb5ce9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy=product_url_101[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0bbeaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets scrape the details of first Table:\n",
    "ShoeName1=[]   \n",
    "Category1=[]\n",
    "Price1=[]\n",
    "\n",
    "start=0\n",
    "end=7\n",
    "for page in range(start,end):\n",
    "    cat=driver.find_elements(By.XPATH,'//p[@class=\"mb-0 p-heading-small pb-1\"]')\n",
    "    for i in cat:\n",
    "        Category1.append(i.text)\n",
    "    next_button=driver.find_element(By.XPATH,'//a[@class=\"page-link next\"]')\n",
    "    next_button.click()\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "884d3be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Category\n",
    "\n",
    "start=0\n",
    "end=7\n",
    "for page in range(start,end):\n",
    "    name=driver.find_elements(By.XPATH,'//p[@class=\"product-tile-product-name font-bold mb-0\"]')\n",
    "    for i in name:\n",
    "        ShoeName1.append(i.text)\n",
    "    next_button=driver.find_element(By.XPATH,'//a[@class=\"page-link next\"]')\n",
    "    next_button.click()\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59d96d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Price\n",
    "\n",
    "Price1=[]\n",
    "start=0\n",
    "end=7\n",
    "for page in range(start,end):\n",
    "    pr=driver.find_elements(By.XPATH,'//span[@class=\"value p-heading mobile-view-global product-list-price-mobile list-price\"]')\n",
    "    for i in pr:\n",
    "        Price1.append(i.text.replace('â‚¹','').replace(',',''))\n",
    "    next_button=driver.find_element(By.XPATH,'//a[@class=\"page-link next\"]')\n",
    "    next_button.click()\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "31ebc7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets scrap the data which is inside of URl     \n",
    "\n",
    "Count_of_sizes1=[]  \n",
    "Reviews1=[]\n",
    "Size1=[]            \n",
    "Star1=[]\n",
    "color1=[]\n",
    "color2=[]\n",
    "color3=[]\n",
    "color4=[]\n",
    "color5=[]\n",
    "Quality1=[]\n",
    "Comfort1=[]\n",
    "Code_Men=[]\n",
    "no_of_colors1=[]\n",
    "\n",
    "for url in product_url_101:\n",
    "    driver.get(url)\n",
    "    time.sleep(2) \n",
    "\n",
    "#Count_of_size;         \n",
    "\n",
    "    try:\n",
    "        size=driver.find_element(By.XPATH,'//div[@class=\"select-size\"]')\n",
    "        Count_of_sizes11.append(size.text.count(' '))\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Count_of_sizes11.append(np.nan)\n",
    "        \n",
    "    \n",
    "    \n",
    "#Size available---------------       \n",
    "        \n",
    "    try:\n",
    "        size1=driver.find_element(By.XPATH,'//div[@class=\"select-size\"]')\n",
    "        Size11.append(size1.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Size11.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "#Star rating----------------------------\n",
    "    try:\n",
    "        star=driver.find_element(By.XPATH,'//div[@class=\"pr-snippet-stars-reco-inline pr-snippet-minimal\"]/div/div/div/div/div[2]')\n",
    "        Star11.append(star.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Star11.append(np.nan)\n",
    "        \n",
    "        \n",
    "#Color1----------------------------\n",
    "    try:\n",
    "        col1=driver.find_element(By.XPATH,'//div[@class=\"attributes pl-0\"]/div[2]/div/div/button[1]')\n",
    "        color11.append(col1.get_attribute(\"aria-describedby\").strip())\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color11.append(np.nan)\n",
    "        \n",
    "        \n",
    "#Color2----------------------------\n",
    "\n",
    "    try:\n",
    "        col2=driver.find_element(By.XPATH,'//div[@class=\"attributes pl-0\"]/div[2]/div/div/button[2]')\n",
    "        color22.append(col2.get_attribute(\"aria-describedby\").strip())\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color22.append(np.nan)\n",
    "        \n",
    "        \n",
    "##Color3----------------------------\n",
    "\n",
    "    try:\n",
    "        col3=driver.find_element(By.XPATH,'//div[@class=\"attributes pl-0\"]/div[2]/div/div/button[3]')\n",
    "        color33.append(col3.get_attribute(\"aria-describedby\").strip())\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color33.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "##Color4----------------------------\n",
    "\n",
    "    try:\n",
    "        col4=driver.find_element(By.XPATH,'//div[@class=\"attributes pl-0\"]/div[2]/div/div/button[4]')\n",
    "        color44.append(col4.get_attribute(\"aria-describedby\").strip())\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color44.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "##Color5----------------------------\n",
    "\n",
    "    try:\n",
    "        col5=driver.find_element(By.XPATH,'//div[@class=\"attributes pl-0\"]/div[2]/div/div/button[5]')\n",
    "        color55.append(col5.get_attribute(\"aria-describedby\").strip())\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color55.append(np.nan)\n",
    "        \n",
    "          \n",
    "#Reviews Count-----------------#\n",
    "\n",
    "    try:\n",
    "        rev=driver.find_element(By.XPATH,'//div[@class=\"pr-snippet-stars-reco-inline pr-snippet-minimal\"]/div/div/div[2]/a[1]')\n",
    "        Reviews11.append(rev.text.split(' ')[0])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Reviews11.append(np.nan)\n",
    "        \n",
    "        \n",
    "#Product Code-----------------------\n",
    "    try:\n",
    "        code=driver.find_element(By.XPATH,'//div[@class=\"attributes pl-0\"]/div[2]/div/div/div/div[2]/span[2]/span')\n",
    "        Code_women.append(code.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Code_women.append(np.nan)\n",
    "        \n",
    "        \n",
    "#Quality-------------------------------------------\n",
    "    try:\n",
    "        qu=driver.find_element(By.XPATH,'//section[@class=\"pr-review-snapshot-block pr-review-snapshot-block-pros\"]/dl/dd[2]/button/span[1]')\n",
    "        Quality11.append(qu.text)\n",
    "    except NoSuchElementException:\n",
    "        Quality11.append(np.nan)\n",
    "        \n",
    "        \n",
    "#Comfort-------------------------------------------\n",
    "    try:\n",
    "        co=driver.find_element(By.XPATH,'//section[@class=\"pr-review-snapshot-block pr-review-snapshot-block-pros\"]/dl/dd/button/span[1]')\n",
    "        Comfort11.append(co.text)\n",
    "    except NoSuchElementException:\n",
    "        Comfort11.append(np.nan)\n",
    "        \n",
    "        \n",
    "#No of Colors--------------------\n",
    "\n",
    "    try:\n",
    "        color=driver.find_element(By.XPATH,'//div[@class=\"attributes pl-0\"]/div[2]/div/div')\n",
    "        a=color.text.count('\\n')\n",
    "        no_of_colors11.append(str(a) +' colors')\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        no_of_colors11.append(np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bacedd9",
   "metadata": {},
   "source": [
    "**Lets Scrape the details of Women's Shoe:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ad0ffc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.skechers.in/women/footwear?start=0&sz=24')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f80981f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets scrape all the product url of all \"Womens\" Shoes:\n",
    "\n",
    "product_url_201=[]   \n",
    "\n",
    "start=0\n",
    "end=7\n",
    "\n",
    "for page in range(start,end):\n",
    "    url=driver.find_elements(By.XPATH,'//a[@class=\"link p-heading\"]')\n",
    "    for i in url:\n",
    "        product_url_201.append(i.get_attribute('href'))\n",
    "    next_button=driver.find_element(By.XPATH,'//a[@class=\"page-link next\"]')\n",
    "    next_button.click()\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69bde797",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_url_201=product_url_201[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3173fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets scrape the details of first Table:\n",
    "ShoeName11=[]   \n",
    "Category11=[]  \n",
    "\n",
    "start=0\n",
    "end=7\n",
    "for page in range(start,end):\n",
    "    cat=driver.find_elements(By.XPATH,'//p[@class=\"mb-0 p-heading-small pb-1\"]')\n",
    "    for i in cat:\n",
    "        Category11.append(i.text)\n",
    "    next_button=driver.find_element(By.XPATH,'//a[@class=\"page-link next\"]')\n",
    "    next_button.click()\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1582eba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=0\n",
    "end=7\n",
    "for page in range(start,end):\n",
    "    name=driver.find_elements(By.XPATH,'//p[@class=\"product-tile-product-name font-bold mb-0\"]')\n",
    "    for i in name:\n",
    "        ShoeName11.append(i.text)\n",
    "    next_button=driver.find_element(By.XPATH,'//a[@class=\"page-link next\"]')\n",
    "    next_button.click()\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2cb26dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Price11=[]\n",
    "start=0\n",
    "end=7\n",
    "for page in range(start,end):\n",
    "    pr=driver.find_elements(By.XPATH,'//span[@class=\"value p-heading mobile-view-global product-list-price-mobile list-price\"]')\n",
    "    for i in pr:\n",
    "        Price11.append(i.text.replace('â‚¹','').replace(',',''))\n",
    "    next_button=driver.find_element(By.XPATH,'//a[@class=\"page-link next\"]')\n",
    "    next_button.click()\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ded35b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets scrap the data which is inside of URl     \n",
    "\n",
    "Count_of_sizes11=[]  \n",
    "Reviews11=[]\n",
    "Size11=[]            \n",
    "Star11=[]\n",
    "color11=[]\n",
    "color22=[]\n",
    "color33=[]\n",
    "color44=[]\n",
    "color55=[]\n",
    "Quality11=[]\n",
    "Comfort11=[]\n",
    "Code_women=[]\n",
    "no_of_colors11=[]\n",
    "\n",
    "for url in product_url_201:\n",
    "    driver.get(url)\n",
    "    time.sleep(2) \n",
    "\n",
    "#Count_of_size;         \n",
    "\n",
    "    try:\n",
    "        size=driver.find_element(By.XPATH,'//div[@class=\"select-size\"]')\n",
    "        Count_of_sizes11.append(size.text.count(' '))\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Count_of_sizes11.append(np.nan)\n",
    "        \n",
    "    \n",
    "    \n",
    "#Size available---------------       \n",
    "        \n",
    "    try:\n",
    "        size1=driver.find_element(By.XPATH,'//div[@class=\"select-size\"]')\n",
    "        Size11.append(size1.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Size11.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "#Star rating----------------------------\n",
    "    try:\n",
    "        star=driver.find_element(By.XPATH,'//div[@class=\"pr-snippet-stars-reco-inline pr-snippet-minimal\"]/div/div/div/div/div[2]')\n",
    "        Star11.append(star.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Star11.append(np.nan)\n",
    "        \n",
    "        \n",
    "#Color1----------------------------\n",
    "    try:\n",
    "        col1=driver.find_element(By.XPATH,'//div[@class=\"attributes pl-0\"]/div[2]/div/div/button[1]')\n",
    "        color11.append(col1.get_attribute(\"aria-describedby\").strip())\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color11.append(np.nan)\n",
    "        \n",
    "        \n",
    "#Color2----------------------------\n",
    "\n",
    "    try:\n",
    "        col2=driver.find_element(By.XPATH,'//div[@class=\"attributes pl-0\"]/div[2]/div/div/button[2]')\n",
    "        color22.append(col2.get_attribute(\"aria-describedby\").strip())\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color22.append(np.nan)\n",
    "        \n",
    "        \n",
    "##Color3----------------------------\n",
    "\n",
    "    try:\n",
    "        col3=driver.find_element(By.XPATH,'//div[@class=\"attributes pl-0\"]/div[2]/div/div/button[3]')\n",
    "        color33.append(col3.get_attribute(\"aria-describedby\").strip())\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color33.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "##Color4----------------------------\n",
    "\n",
    "    try:\n",
    "        col4=driver.find_element(By.XPATH,'//div[@class=\"attributes pl-0\"]/div[2]/div/div/button[4]')\n",
    "        color44.append(col4.get_attribute(\"aria-describedby\").strip())\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color44.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "##Color5----------------------------\n",
    "\n",
    "    try:\n",
    "        col5=driver.find_element(By.XPATH,'//div[@class=\"attributes pl-0\"]/div[2]/div/div/button[5]')\n",
    "        color55.append(col5.get_attribute(\"aria-describedby\").strip())\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color55.append(np.nan)\n",
    "        \n",
    "          \n",
    "#Reviews Count-----------------#\n",
    "\n",
    "    try:\n",
    "        rev=driver.find_element(By.XPATH,'//div[@class=\"pr-snippet-stars-reco-inline pr-snippet-minimal\"]/div/div/div[2]/a[1]')\n",
    "        Reviews11.append(rev.text.split(' ')[0])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Reviews11.append(np.nan)\n",
    "        \n",
    "        \n",
    "#Product Code-----------------------\n",
    "    try:\n",
    "        code=driver.find_element(By.XPATH,'//div[@class=\"attributes pl-0\"]/div[2]/div/div/div/div[2]/span[2]/span')\n",
    "        Code_women.append(code.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Code_women.append(np.nan)\n",
    "        \n",
    "        \n",
    "#Quality-------------------------------------------\n",
    "    try:\n",
    "        qu=driver.find_element(By.XPATH,'//section[@class=\"pr-review-snapshot-block pr-review-snapshot-block-pros\"]/dl/dd[2]/button/span[1]')\n",
    "        Quality11.append(qu.text)\n",
    "    except NoSuchElementException:\n",
    "        Quality11.append(np.nan)\n",
    "        \n",
    "        \n",
    "#Comfort-------------------------------------------\n",
    "    try:\n",
    "        co=driver.find_element(By.XPATH,'//section[@class=\"pr-review-snapshot-block pr-review-snapshot-block-pros\"]/dl/dd/button/span[1]')\n",
    "        Comfort11.append(co.text)\n",
    "    except NoSuchElementException:\n",
    "        Comfort11.append(np.nan)\n",
    "        \n",
    "        \n",
    "#No of Colors--------------------\n",
    "\n",
    "    try:\n",
    "        color=driver.find_element(By.XPATH,'//div[@class=\"attributes pl-0\"]/div[2]/div/div')\n",
    "        a=color.text.count('\\n')\n",
    "        no_of_colors11.append(str(a) +' colors')\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        no_of_colors11.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6c506d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here Im merging the Mens and Womens shoes details\n",
    "\n",
    "shoe=ShoeName1+ShoeName11\n",
    "price=Price1+Price11\n",
    "count=Count_of_sizes1+Count_of_sizes11\n",
    "rev=Reviews1+Reviews11\n",
    "size=Size1+Size11\n",
    "star=Star1+Star11\n",
    "co1=color1+color11\n",
    "co2=color2+color22\n",
    "co3=color3+color33\n",
    "co4=color4+color44\n",
    "co5=color5+color55\n",
    "code=Code_Men+Code_women\n",
    "Qua=Quality1+Quality11\n",
    "comf=Comfort1+Comfort11\n",
    "ncs=no_of_colors1+no_of_colors11\n",
    "cate=Category1+Category11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e95f7dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Table No 1\n",
    "df1=pd.DataFrame({'ShoeName':shoe,'Category':cate,'no_of_colors':ncs,'Price':price})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c02e8a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the file into CSV\n",
    "df1.to_csv('Skechers_table1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "78411e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Table No 2\n",
    "df2=pd.DataFrame({'count_of_size':count,'Color1':co1,'Color2':co2,'Color3':co3,'Color4':co4,'Color5':co5,'Product_code':code})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "435ff520",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the file in CSV\n",
    "df2.to_csv('Skechers_table2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aaf147a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Table No 3---\n",
    "df3=pd.DataFrame({'reviews':rev, 'Size':size, 'comfort':comf, 'quality':Qua, 'star_rating':star})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cb42e690",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the file in CSV\n",
    "df3.to_csv('Skechers_table3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eda8726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95cc08e3",
   "metadata": {},
   "source": [
    "# 2.Columbia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28146293",
   "metadata": {},
   "source": [
    "**First i will divide the Scrapping into two types, 1.Men and 2.Women**\n",
    "\n",
    "#First i will scrap the details of men Shoes\n",
    "\n",
    "#Creating a empty list and at last we will append the scrapped values into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e938bb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening the webdriver \n",
    "driver=webdriver.Chrome()     \n",
    "driver.get('https://www.columbia.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fe8309ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First will Scrap the Mens Shoes\n",
    "\n",
    "Men=driver.find_element(By.XPATH,'/html/body/header/div/div/div[2]/div/div[2]/div/div/ul/li[2]')\n",
    "Men.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "06610ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "men_shoes=driver.find_element(By.XPATH,'/html/body/div[5]/div[1]/aside/div/div[1]/div[3]/ul/li/div/ul/li[4]/a')\n",
    "men_shoes.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "768af6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scroll down the page a bit,\n",
    "\n",
    "for _ in range(150):\n",
    "    driver.execute_script('window.scrollBy(0,4000)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bb8d36",
   "metadata": {},
   "source": [
    "Columbia's website features a limited selection of shoes. To comprehensively gather information, I have initiated the process of web scraping to capture details on all available shoes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "089827b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets scrap the URL of all the shoes \n",
    "\n",
    "Product_url1=[]\n",
    "    \n",
    "url=driver.find_elements(By.XPATH,'//a[@class=\"single-product\"]')\n",
    "for i in url:\n",
    "    Product_url1.append(i.get_attribute(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dfbe1f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ShoeName1=[]   \n",
    "\n",
    "#Shoename:\n",
    "name=driver.find_elements(By.XPATH,'//div[@class=\"product-layout product-item product-grid col-xs-6 col-md-4 col-lg-3\"]/div/div[2]/div/h4/span/a')\n",
    "for a in name:\n",
    "    ShoeName1.append(a.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "66edf8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Price11=[]\n",
    "Count_of_sizes11=[]\n",
    "Reviews11=[]\n",
    "Size11=[]\n",
    "Star11=[]\n",
    "color11=[]\n",
    "color22=[]\n",
    "color33=[]\n",
    "color44=[]\n",
    "color55=[]\n",
    "Code_Men1=[]\n",
    "Quality11=[]\n",
    "Comfort11=[]\n",
    "no_of_colors11=[]\n",
    "\n",
    "for url in Product_url1:\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    \n",
    "#Price:\n",
    "\n",
    "    try:\n",
    "        price=driver.find_element(By.XPATH,'//ul[@class=\"list-unstyled price-container\"]/li/h2')\n",
    "        Price11.append(price.text.replace(',',''))\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Price11.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "#Count_of_size;         \n",
    "\n",
    "    try:\n",
    "        size=driver.find_element(By.XPATH,'//div[@class=\"option-main\"]')\n",
    "        Count_of_sizes11.append(size.text.count('\\n'))\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Count_of_sizes11.append(np.nan)\n",
    "        \n",
    "\n",
    "#Reviews Count-----------------#\n",
    "\n",
    "    try:\n",
    "        rev=driver.find_element(By.XPATH,'//div[@class=\"review-star-block\"]/p[1]/span[1]')\n",
    "        Reviews11.append(rev.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Reviews11.append(np.nan)\n",
    "        \n",
    "        \n",
    "#Size available---------------       \n",
    "        \n",
    "    try:\n",
    "        size1=driver.find_element(By.XPATH,'//div[@class=\"option-main\"]')\n",
    "        a=size1.text.replace('\\n',',')\n",
    "        Size11.append(a.replace('Left',''))\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Size11.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "#Star rating----------------------------\n",
    "    try:\n",
    "        star=driver.find_element(By.XPATH,'//div[@class=\"review-rating-block\"]/div/div[1]')\n",
    "        Star11.append(star.text.split('of')[0])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Star11.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "#Color1----------------------------\n",
    "    try:\n",
    "        col1=driver.find_element(By.XPATH,'//label[@class=\"control-label-name\"]')\n",
    "        color11.append(col1.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color11.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "#Color2----------------------------\n",
    "\n",
    "    try:\n",
    "        col2=driver.find_element(By.XPATH,'//div[@class=\"grop-product-product\"]/a[2]')\n",
    "        color22.append(col2.get_attribute(\"data-color\").strip())\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color22.append(np.nan)\n",
    "        \n",
    "        \n",
    "##Color3----------------------------\n",
    "\n",
    "    try:\n",
    "        col3=driver.find_element(By.XPATH,'//div[@class=\"grop-product-product\"]/a[3]')\n",
    "        color33.append(col3.get_attribute(\"data-color\").strip())\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color33.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "##Color4----------------------------\n",
    "\n",
    "    try:\n",
    "        col4=driver.find_element(By.XPATH,'//div[@class=\"grop-product-product\"]/a[4]')\n",
    "        color44.append(col4.get_attribute(\"data-color\").strip())\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color44.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "##Color5----------------------------\n",
    "\n",
    "    try:\n",
    "        col5=driver.find_element(By.XPATH,'//div[@class=\"grop-product-product\"]/a[5]')\n",
    "        color55.append(col5.get_attribute(\"data-color\").strip())\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color55.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "#Product Code-------------\n",
    "\n",
    "    try:\n",
    "        code=driver.find_element(By.XPATH,'//div[@class=\"col-sm-5\"]/p[1]')\n",
    "        Code_Men1.append(code.text.split(':')[1])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Code_Men1.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#Quality------------------------------------------- #No Quality Available\n",
    "    try:\n",
    "        qu=driver.find_element(By.XPATH,'//div[@class=\"grop-product-product\"]/a[7]')\n",
    "        Quality11.append(qu.text.count('\\n'))\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Quality11.append(np.nan)\n",
    "        \n",
    "        \n",
    "#Comfort-------------------------------------------#No Comfort ratings available\n",
    "    try:\n",
    "        co=driver.find_element(By.XPATH,'//div[@class=\"grop-product-product\"]/a[7]')\n",
    "        Comfort11.append(co.text.count('\\n'))\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Comfort11.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "#Number of Colors----------------------------\n",
    "    try:\n",
    "        num=driver.find_element(By.XPATH,'//div[@class=\"grop-product\"]')\n",
    "        a=num.get_attribute(\"href\")\n",
    "        no_of_colors11.append(a)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        no_of_colors11.append(np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04060dd0",
   "metadata": {},
   "source": [
    "**Lets Scrape the details of Women's Shoe:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a8cac95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First will Scrap the Women's Shoes\n",
    "\n",
    "Women=driver.find_element(By.XPATH,'/html/body/header/div/div/div[2]/div/div[2]/div/div/ul/li[3]/a')\n",
    "Women.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eceb21df",
   "metadata": {},
   "outputs": [],
   "source": [
    "women_shoes=driver.find_element(By.XPATH,'/html/body/div[5]/div[1]/aside/div/div[1]/div[3]/ul/li/div/ul/li[4]/a')\n",
    "women_shoes.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fa7caa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scroll down the page a bit,\n",
    "\n",
    "for _ in range(150):\n",
    "    driver.execute_script('window.scrollBy(0,4000)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3629f410",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets scrap the URL of all the shoes \n",
    "\n",
    "Product_url2=[]\n",
    "    \n",
    "url=driver.find_elements(By.XPATH,'//a[@class=\"single-product\"]')\n",
    "for i in url:\n",
    "    Product_url2.append(i.get_attribute(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d9357243",
   "metadata": {},
   "outputs": [],
   "source": [
    "ShoeName11=[]   \n",
    "\n",
    "\n",
    "#Shoename:\n",
    "name=driver.find_elements(By.XPATH,'//div[@class=\"product-layout product-item product-grid col-xs-6 col-md-4 col-lg-3\"]/div/div[2]/div/h4/span/a')\n",
    "for a in name:\n",
    "    ShoeName11.append(a.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1da60099",
   "metadata": {},
   "outputs": [],
   "source": [
    "Price111=[]\n",
    "Count_of_sizes111=[]\n",
    "Reviews111=[]\n",
    "Size111=[]\n",
    "Star111=[]\n",
    "color111=[]\n",
    "color222=[]\n",
    "color333=[]\n",
    "color444=[]\n",
    "color555=[]\n",
    "Code_women=[]\n",
    "Quality111=[]\n",
    "Comfort111=[]\n",
    "no_of_colors111=[]\n",
    "\n",
    "for url in Product_url2:\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    \n",
    "#Price:\n",
    "\n",
    "    try:\n",
    "        price=driver.find_element(By.XPATH,'//ul[@class=\"list-unstyled price-container\"]/li/h2')\n",
    "        Price111.append(price.text.replace(',',''))\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Price111.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "#Count_of_size;         \n",
    "\n",
    "    try:\n",
    "        size=driver.find_element(By.XPATH,'//div[@class=\"option-main\"]')\n",
    "        Count_of_sizes111.append(size.text.count('\\n'))\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Count_of_sizes111.append(np.nan)\n",
    "        \n",
    "\n",
    "#Reviews Count-----------------#\n",
    "\n",
    "    try:\n",
    "        rev=driver.find_element(By.XPATH,'//div[@class=\"review-star-block\"]/p[1]/span[1]')\n",
    "        Reviews111.append(rev.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Reviews111.append(np.nan)\n",
    "        \n",
    "        \n",
    "#Size available---------------       \n",
    "        \n",
    "    try:\n",
    "        size1=driver.find_element(By.XPATH,'//div[@class=\"option-main\"]')\n",
    "        a=size1.text.replace('\\n',',')\n",
    "        Size111.append(a.replace('Left',''))\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Size111.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "#Star rating----------------------------\n",
    "    try:\n",
    "        star=driver.find_element(By.XPATH,'//div[@class=\"review-rating-block\"]/div/div[1]')\n",
    "        Star111.append(star.text.split('of')[0])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Star111.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "#Color1----------------------------\n",
    "    try:\n",
    "        col1=driver.find_element(By.XPATH,'//label[@class=\"control-label-name\"]')\n",
    "        color111.append(col1.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color111.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "#Color2----------------------------\n",
    "\n",
    "    try:\n",
    "        col2=driver.find_element(By.XPATH,'//div[@class=\"grop-product-product\"]/a[2]')\n",
    "        color222.append(col2.get_attribute(\"data-color\").strip())\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color222.append(np.nan)\n",
    "        \n",
    "        \n",
    "##Color3----------------------------\n",
    "\n",
    "    try:\n",
    "        col3=driver.find_element(By.XPATH,'//div[@class=\"grop-product-product\"]/a[3]')\n",
    "        color333.append(col3.get_attribute(\"data-color\").strip())\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color333.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "##Color4----------------------------\n",
    "\n",
    "    try:\n",
    "        col4=driver.find_element(By.XPATH,'//div[@class=\"grop-product-product\"]/a[4]')\n",
    "        color444.append(col4.get_attribute(\"data-color\").strip())\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color444.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "##Color5----------------------------\n",
    "\n",
    "    try:\n",
    "        col5=driver.find_element(By.XPATH,'//div[@class=\"grop-product-product\"]/a[5]')\n",
    "        color555.append(col5.get_attribute(\"data-color\").strip())\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color555.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "#Product Code-------------\n",
    "\n",
    "    try:\n",
    "        code=driver.find_element(By.XPATH,'//div[@class=\"col-sm-5\"]/p[1]')\n",
    "        Code_women.append(code.text.split(':')[1])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Code_women.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#Quality------------------------------------------- #No Quality Available\n",
    "    try:\n",
    "        qu=driver.find_element(By.XPATH,'//div[@class=\"grop-product-product\"]/a[7]')\n",
    "        Quality111.append(qu.text.count('\\n'))\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Quality111.append(np.nan)\n",
    "        \n",
    "        \n",
    "#Comfort-------------------------------------------#No Comfort ratings available\n",
    "    try:\n",
    "        co=driver.find_element(By.XPATH,'//div[@class=\"grop-product-product\"]/a[7]')\n",
    "        Comfort111.append(co.text.count('\\n'))\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Comfort111.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "#Number of Colors----------------------------\n",
    "    try:\n",
    "        num=driver.find_element(By.XPATH,'//div[@class=\"grop-product\"]')\n",
    "        a=num.get_attribute(\"href\")\n",
    "        no_of_colors111.append(a)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        no_of_colors111.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "14fcb02a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To ensure balance in the equation, I am appending an equal number of men and women in the category columns for Colombia brand, as there are no specific details available for scraping.\n",
    "\n",
    "Category11=['Men']*73\n",
    "len(Category11)\n",
    "\n",
    "\n",
    "Category111=['Women']*57\n",
    "len(Category111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b17cfcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "sh=ShoeName1+ShoeName11\n",
    "p=Price11+Price111\n",
    "cs=Count_of_sizes11+Count_of_sizes111\n",
    "r=Reviews11+Reviews111\n",
    "si=Size11+Size111\n",
    "st=Star11+Star111\n",
    "c1=color11+color111\n",
    "c2=color22+color222\n",
    "c3=color33+color333\n",
    "c4=color44+color444\n",
    "c5=color55+color555\n",
    "cod=Code_Men1+Code_women\n",
    "Qu=Quality11+Quality111\n",
    "com=Comfort11+Comfort111\n",
    "nc=no_of_colors11+no_of_colors111\n",
    "ct=Category11+Category111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1a207ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Table No 1\n",
    "df1=pd.DataFrame({'ShoeName':sh,'Category':ct,'no_of_colors':nc,'Price':p})\n",
    "\n",
    "#Saving it as CSV File\n",
    "df1.to_csv('Colombia_table1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3bfc3f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Table No 2\n",
    "\n",
    "df2=pd.DataFrame({'count_of_size':cs,'Color1':c1,'Color2':c2,'Color3':c3,'Color4':c4,'Color5':c5,'Product_code':cod})\n",
    "\n",
    "#Saving it as CSV File\n",
    "df2.to_csv('Colombia_table2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7f410f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Table No 3\n",
    "df3=pd.DataFrame({'reviews':r, 'Size':si, 'comfort':com, 'quality':Qu, 'star_rating':st})\n",
    "\n",
    "#Saving it as CSV File\n",
    "df3.to_csv('Colombia_table3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a82e8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "773a5a86",
   "metadata": {},
   "source": [
    "# 3.Adidas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a43db9",
   "metadata": {},
   "source": [
    "First i will divide the Scrapping into two types, 1.Men and 2.Women\n",
    "\n",
    "#First i will scrap the details of men Shoes\n",
    "\n",
    "#Creating a empty list and at last we will append the scrapped values into it.\n",
    "\n",
    "#Scrapping of 150 shoes for men and Women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7613bd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome()     #United States\n",
    "driver.get('https://www.adidas.com/us')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "aabaa0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First will Scrap the Mens Shoes\n",
    "\n",
    "Men=driver.find_element(By.XPATH,'/html/body/div[1]/div[2]/div[2]/div/header/div[2]/div/ul/li[1]/a')\n",
    "Men.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7c7fece2",
   "metadata": {},
   "outputs": [],
   "source": [
    "men_shoes=driver.find_element(By.XPATH,'/html/body/div[2]/div/div[1]/div[1]/div/div/div[4]/div[3]/section/div[2]/div/div[2]/div/div/a')\n",
    "men_shoes.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7d6f1c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets scrape all the product url \n",
    "\n",
    "product_url_102=[]\n",
    "start=0\n",
    "end=4\n",
    "for page in range(start,end):\n",
    "    url=driver.find_elements(By.XPATH,'//*[@id=\"main-content\"]/div/div[3]/div/div/div/div[1]/div/div/div/div/div/div/div/div/a')\n",
    "    for i in url:\n",
    "        product_url_102.append(i.get_attribute('href'))\n",
    "    next_button=driver.find_element(By.XPATH,'/html/body/div[2]/div/div[1]/div[1]/div/div/div[5]/div/div[3]/div/div/div/div[2]/div/div/div/div[4]/a/span')\n",
    "    next_button.click()\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b0d0166d",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_url_102=product_url_102[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5f56862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ShoeName1=[]   \n",
    "Category1=[]  \n",
    "no_of_colors1=[]  \n",
    "Price1=[] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f1ed3a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ShoeName:\n",
    "\n",
    "start=0\n",
    "end=4\n",
    "for page in range(start,end):\n",
    "    name=driver.find_elements(By.XPATH,'//div[@class=\"glass-product-card__details\"]/p[1]')\n",
    "    for i in name:\n",
    "        ShoeName1.append(i.text)\n",
    "    next_button=driver.find_element(By.XPATH,'/html/body/div[2]/div/div[1]/div[1]/div/div/div[5]/div/div[3]/div/div/div/div[2]/div/div/div/div[4]/a/span')\n",
    "    next_button.click()\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e779c808",
   "metadata": {},
   "outputs": [],
   "source": [
    "#no_of_colors\n",
    "\n",
    "start=0\n",
    "end=4\n",
    "for page in range(start,end):\n",
    "    col=driver.find_elements(By.XPATH,'//div[@class=\"glass-product-card__details\"]/p[3]/span[1]')\n",
    "    for i in col:\n",
    "        no_of_colors1.append(i.text)\n",
    "    next_button=driver.find_element(By.XPATH,'/html/body/div[2]/div/div[1]/div[1]/div/div/div[5]/div/div[3]/div/div/div/div[2]/div/div/div/div[4]/a/span')\n",
    "    next_button.click()\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "045deea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the dataset, a significant portion of shoes is categorized as basketball, golf shoes, running shoes, and more. Therefore, while scraping data from the men's section, we are appropriately assigning the corresponding value as 'men'.\n",
    "Category1=['Men']*150\n",
    "\n",
    "len(Category1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "73af36ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets prepare the Table 2 for Men\n",
    "\n",
    "#Lets scrap the data which is inside of URl     \n",
    "Pri=[]\n",
    "Count_of_sizes1=[]\n",
    "Reviews1=[]\n",
    "Size1=[]\n",
    "Star1=[]\n",
    "color1=[]\n",
    "color2=[]\n",
    "color3=[]\n",
    "color4=[]\n",
    "color5=[]\n",
    "Quality1=[]\n",
    "Comfort1=[]\n",
    "\n",
    "for url in product_url_102:\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    try:\n",
    "        price=driver.find_element(By.XPATH,'//div[@class=\"product-price___2Mip5 gl-vspace\"]/div/div/div')\n",
    "        Pri.append(price.text.replace('$',''))\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Pri.append(np.nan)\n",
    "        \n",
    "\n",
    "#Count_of_size;         #SIZE is not loading from Website website issue\n",
    "\n",
    "    try:\n",
    "        size=driver.find_element(By.XPATH,'//div[@class=\"gl-callout gl-callout--urgent error___3RWqA gl-vspace\"]')\n",
    "        Count_of_sizes1.append(size.text.count('\\n'))\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Count_of_sizes1.append(np.nan)\n",
    "        \n",
    "        \n",
    "#Reviews Count-----------------#\n",
    "\n",
    "    try:\n",
    "        rev=driver.find_element(By.XPATH,'//div[@class=\"reviews-header___1kFSZ\"]/h2')\n",
    "        Reviews1.append(rev.text.split(' ')[1].split('(')[1].split(')')[0])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Reviews1.append(np.nan)\n",
    "\n",
    "#Size available---------------       \n",
    "        \n",
    "    try:\n",
    "        size1=driver.find_element(By.XPATH,'//div[@class=\"gl-callout gl-callout--urgent error___3RWqA gl-vspace\"]')\n",
    "        Size1.append(size1.text.replace('\\n',','))\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Size1.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "#Star rating----------------------------\n",
    "    try:\n",
    "        star=driver.find_element(By.XPATH,'//div[@class=\"reviews-header___1kFSZ\"]/div/span')\n",
    "        Star1.append(star.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Star1.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "#Color1----------------------------\n",
    "    try:\n",
    "        col1=driver.find_element(By.XPATH,'//div[@class=\"color-label___2hXaD\"]')\n",
    "        color1.append(col1.text.split('/')[0])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color1.append(np.nan)\n",
    "      \n",
    "    \n",
    "##Color2----------------------------\n",
    "\n",
    "    try:\n",
    "        col2=driver.find_element(By.XPATH,'//div[@class=\"color-chooser-grid___1ZBx_\"]/a[2]/img')\n",
    "        color2.append(col2.get_attribute(\"alt\").strip().split(':')[1].split('/')[0])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color2.append(np.nan)\n",
    "        \n",
    "        \n",
    "##Color3----------------------------\n",
    "\n",
    "    try:\n",
    "        col3=driver.find_element(By.XPATH,'//div[@class=\"color-chooser-grid___1ZBx_\"]/a[3]/img')\n",
    "        color3.append(col3.get_attribute(\"alt\").strip().split(':')[1].split('/')[0])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color3.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "##Color4----------------------------\n",
    "\n",
    "    try:\n",
    "        col4=driver.find_element(By.XPATH,'//div[@class=\"color-chooser-grid___1ZBx_\"]/a[4]/img')\n",
    "        color4.append(col4.get_attribute(\"alt\").strip().split(':')[1].split('/')[0])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color4.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "##Color5----------------------------\n",
    "\n",
    "    try:\n",
    "        col5=driver.find_element(By.XPATH,'//div[@class=\"color-chooser-grid___1ZBx_\"]/a[5]/img')\n",
    "        color5.append(col5.get_attribute(\"alt\").strip().split(':')[1].split('/')[0])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color5.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#Quality-------------------------------------------\n",
    "    try:\n",
    "        button1=driver.find_element(By.XPATH,'//*[@id=\"navigation-target-reviews\"]/div/button')\n",
    "        button1.click()\n",
    "        qu=driver.find_element(By.XPATH,'//div[@class=\"sub-ratings___1pAhV\"]/div[2]/div/div[2]/div')\n",
    "        Quality1.append(qu.get_attribute(\"style\").strip().split(';')[1].split(':')[1].split(';')[0])\n",
    "    except NoSuchElementException:\n",
    "        Quality1.append(np.nan)\n",
    "        \n",
    "        \n",
    "#Comfort-------------------------------------------\n",
    "    try:\n",
    "        time.sleep(2)\n",
    "        button1=driver.find_element(By.XPATH,'//*[@id=\"navigation-target-reviews\"]/div/button')\n",
    "        button1.click()\n",
    "        co=driver.find_element(By.XPATH,'//div[@class=\"sub-ratings___1pAhV\"]/div[1]/div/div[2]/div')\n",
    "        Comfort1.append(co.get_attribute(\"style\").strip().split(';')[1].split(':')[1].split(';')[0])\n",
    "    except NoSuchElementException:\n",
    "        Comfort1.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "baedb764",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Product Code for Men:\n",
    "\n",
    "Code_Men=[]\n",
    "\n",
    "for url in product_url_102:\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        button=driver.find_element(By.XPATH,'//*[@id=\"navigation-target-specifications\"]/div/button/div[2]')\n",
    "        button.click()\n",
    "        code=driver.find_element(By.XPATH,'//div[@class=\"bullets___3bsSs\"]/ul[2]/li[5]')\n",
    "        Code_Men.append(code.text.split(':')[1])\n",
    "        \n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Code_Men.append(np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232197a5",
   "metadata": {},
   "source": [
    "**Lets Scrape the details of Women's Shoe:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "68c2a8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome()     #United States\n",
    "driver.get('https://www.adidas.com/us')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "dd507c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First will Scrap the Mens Shoes\n",
    "\n",
    "women=driver.find_element(By.XPATH,'/html/body/div[1]/div[2]/div[2]/div/header/div[2]/div/ul/li[2]/a')\n",
    "women.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a4d8e7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "women_shoes=driver.find_element(By.XPATH,'/html/body/div[2]/div/div[1]/div[1]/div/div/div[4]/div[3]/section/div[2]/div/div[2]/div/div/a')\n",
    "women_shoes.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "50982a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets scrape all the product url #Women\n",
    "\n",
    "product_url_200=[]\n",
    "start=0\n",
    "end=4\n",
    "for page in range(start,end):\n",
    "    url=driver.find_elements(By.XPATH,'//*[@id=\"main-content\"]/div/div[3]/div/div/div/div[1]/div/div/div/div/div/div/div/div/a')\n",
    "    for i in url:\n",
    "        product_url_200.append(i.get_attribute('href'))\n",
    "    next_button=driver.find_element(By.XPATH,'/html/body/div[2]/div/div[1]/div[1]/div/div/div[5]/div/div[3]/div/div/div/div[2]/div/div/div/div[4]/a/span')\n",
    "    next_button.click()\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7df3c554",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_url_200=product_url_200[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b4d9e016",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets scrape the details of first Table:\n",
    "ShoeName11=[]   \n",
    "Category11=[]  \n",
    "no_of_colors11=[]  \n",
    "\n",
    "start=0\n",
    "end=4\n",
    "for page in range(start,end):\n",
    "    name=driver.find_elements(By.XPATH,'//div[@class=\"glass-product-card__details\"]/p[1]')\n",
    "    for i in name:\n",
    "        ShoeName11.append(i.text)\n",
    "    next_button=driver.find_element(By.XPATH,'/html/body/div[2]/div/div[1]/div[1]/div/div/div[5]/div/div[3]/div/div/div/div[2]/div/div/div/div[4]/a/span')\n",
    "    next_button.click()\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b543a797",
   "metadata": {},
   "outputs": [],
   "source": [
    "ShoeName11=ShoeName11[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d1af2e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#no_of_colors\n",
    "\n",
    "start=0\n",
    "end=4\n",
    "for page in range(start,end):\n",
    "    col=driver.find_elements(By.XPATH,'//div[@class=\"glass-product-card__details\"]/p[3]/span[1]')\n",
    "    for i in col:\n",
    "        no_of_colors11.append(i.text)\n",
    "    next_button=driver.find_element(By.XPATH,'/html/body/div[2]/div/div[1]/div[1]/div/div/div[5]/div/div[3]/div/div/div/div[2]/div/div/div/div[4]/a/span')\n",
    "    next_button.click()\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0b4ee98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_colors11=no_of_colors11[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "be35da3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the dataset, a significant portion of shoes is categorized as basketball, golf shoes, running shoes, and more. Therefore, while scraping data from the men's section, we are appropriately assigning the corresponding value as 'Women'.\n",
    "\n",
    "Category11=['Women']*150\n",
    "\n",
    "len(Category11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c6084a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets scrap the data which is inside of URl     \n",
    "Pri1=[]\n",
    "Count_of_sizes11=[]\n",
    "Reviews11=[]\n",
    "Size11=[]\n",
    "Star11=[]\n",
    "color11=[]\n",
    "color22=[]\n",
    "color33=[]\n",
    "color44=[]\n",
    "color55=[]\n",
    "Quality11=[]\n",
    "Comfort11=[]\n",
    "\n",
    "for url in product_url_200:\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    try:\n",
    "        price=driver.find_element(By.XPATH,'//div[@class=\"product-price___2Mip5 gl-vspace\"]/div/div/div')\n",
    "        Pri1.append(price.text.replace('$',''))\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Pri1.append(np.nan)\n",
    "        \n",
    "\n",
    "#Count_of_size;         #SIZE is not loading from Website website issue\n",
    "\n",
    "    try:\n",
    "        size=driver.find_element(By.XPATH,'//div[@class=\"gl-callout gl-callout--urgent error___3RWqA gl-vspace\"]')\n",
    "        Count_of_sizes11.append(size.text.count('\\n'))\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Count_of_sizes11.append(np.nan)\n",
    "        \n",
    "        \n",
    "#Reviews Count-----------------#\n",
    "\n",
    "    try:\n",
    "        rev=driver.find_element(By.XPATH,'//div[@class=\"reviews-header___1kFSZ\"]/h2')\n",
    "        Reviews11.append(rev.text.split(' ')[1].split('(')[1].split(')')[0])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Reviews11.append(np.nan)\n",
    "\n",
    "#Size available---------------       \n",
    "        \n",
    "    try:\n",
    "        size1=driver.find_element(By.XPATH,'//div[@class=\"gl-callout gl-callout--urgent error___3RWqA gl-vspace\"]')\n",
    "        Size11.append(size1.text.replace('\\n',','))\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Size11.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "#Star rating----------------------------\n",
    "    try:\n",
    "        star=driver.find_element(By.XPATH,'//div[@class=\"reviews-header___1kFSZ\"]/div/span')\n",
    "        Star11.append(star.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Star11.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "#Color1----------------------------\n",
    "    try:\n",
    "        col1=driver.find_element(By.XPATH,'//div[@class=\"color-label___2hXaD\"]')\n",
    "        color11.append(col1.text.split('/')[0])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color11.append(np.nan)\n",
    "      \n",
    "    \n",
    "##Color2----------------------------\n",
    "\n",
    "    try:\n",
    "        col2=driver.find_element(By.XPATH,'//div[@class=\"color-chooser-grid___1ZBx_\"]/a[2]/img')\n",
    "        color22.append(col2.get_attribute(\"alt\").strip().split(':')[1].split('/')[0])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color22.append(np.nan)\n",
    "        \n",
    "        \n",
    "##Color3----------------------------\n",
    "\n",
    "    try:\n",
    "        col3=driver.find_element(By.XPATH,'//div[@class=\"color-chooser-grid___1ZBx_\"]/a[3]/img')\n",
    "        color33.append(col3.get_attribute(\"alt\").strip().split(':')[1].split('/')[0])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color33.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "##Color4----------------------------\n",
    "\n",
    "    try:\n",
    "        col4=driver.find_element(By.XPATH,'//div[@class=\"color-chooser-grid___1ZBx_\"]/a[4]/img')\n",
    "        color44.append(col4.get_attribute(\"alt\").strip().split(':')[1].split('/')[0])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color44.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "##Color5----------------------------\n",
    "\n",
    "    try:\n",
    "        col5=driver.find_element(By.XPATH,'//div[@class=\"color-chooser-grid___1ZBx_\"]/a[5]/img')\n",
    "        color55.append(col5.get_attribute(\"alt\").strip().split(':')[1].split('/')[0])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color55.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#Quality-------------------------------------------\n",
    "    try:\n",
    "        time.sleep(3)\n",
    "        button1=driver.find_element(By.XPATH,'//*[@id=\"navigation-target-reviews\"]/div/button')\n",
    "        button1.click()\n",
    "        qu=driver.find_element(By.XPATH,'//div[@class=\"sub-ratings___1pAhV\"]/div[2]/div/div[2]/div')\n",
    "        Quality11.append(qu.get_attribute(\"style\").strip().split(';')[1].split(':')[1].split(';')[0])\n",
    "    except NoSuchElementException:\n",
    "        Quality11.append(np.nan)\n",
    "        \n",
    "        \n",
    "#Comfort-------------------------------------------\n",
    "    try:\n",
    "        time.sleep(3)\n",
    "        button1=driver.find_element(By.XPATH,'//*[@id=\"navigation-target-reviews\"]/div/button')\n",
    "        button1.click()\n",
    "        co=driver.find_element(By.XPATH,'//div[@class=\"sub-ratings___1pAhV\"]/div[1]/div/div[2]/div')\n",
    "        Comfort11.append(co.get_attribute(\"style\").strip().split(';')[1].split(':')[1].split(';')[0])\n",
    "    except NoSuchElementException:\n",
    "        Comfort11.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "120143a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Product Code for Women\n",
    "\n",
    "Code_women=[]\n",
    "\n",
    "for url in product_url_200:\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        button=driver.find_element(By.XPATH,'//*[@id=\"navigation-target-specifications\"]/div/button/div[2]')\n",
    "        button.click()\n",
    "        code=driver.find_element(By.XPATH,'//div[@class=\"bullets___3bsSs\"]/ul[2]/li[5]')\n",
    "        Code_women.append(code.text)\n",
    "        time.sleep(3)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Code_women.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c5a49cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets merge the Men and Women columns\n",
    "p=Pri+Pri1\n",
    "Cs=Count_of_sizes1+Count_of_sizes11\n",
    "R=Reviews1+Reviews11\n",
    "si=Size1+Size11\n",
    "st=Star1+Star11\n",
    "c1=color1+color11\n",
    "c2=color2+color22\n",
    "c3=color3+color33\n",
    "c4=color4+color44\n",
    "c5=color5+color55\n",
    "Qu=Quality1+Quality11\n",
    "Co=Comfort1+Comfort11\n",
    "sh=ShoeName1+ShoeName11 \n",
    "Cat=Category1+Category11\n",
    "nc=no_of_colors1+no_of_colors11\n",
    "cod=Code_Men+Code_women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2e427fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Table No 1\n",
    "df1=pd.DataFrame({'ShoeName':sh,'Category':Cat,'no_of_colors':nc,'Price':p})\n",
    "\n",
    "#Saving the file to CSV\n",
    "df1.to_csv('Adidas_table1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "03a2c103",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Table No 2\n",
    "df2=pd.DataFrame({'count_of_size':Cs,'Color1':c1,'Color2':c2,'Color3':c3,'Color4':c4,'Color5':c5,'Product_code':cod})\n",
    "\n",
    "#Saving the file to CSV\n",
    "df2.to_csv('Adidas_table2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e32a629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Table No 3\n",
    "df3=pd.DataFrame({'reviews':R, 'Size':si, 'comfort':Co, 'quality':Qu, 'star_rating':st})\n",
    "\n",
    "#Saving the file to CSV\n",
    "df3.to_csv('Adidas_table3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1eea94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa70d8c9",
   "metadata": {},
   "source": [
    "# 4.PUMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46872324",
   "metadata": {},
   "source": [
    "**First i will divide the Scrapping into two types, 1.Men and 2.Women**\n",
    "\n",
    "#First i will scrap the details of men Shoes\n",
    "\n",
    "#Creating a empty list and at last we will append the scrapped values into it.\n",
    "\n",
    "#Scrapping of 150 shoes for men and Women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "da05b7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome()     #United States\n",
    "driver.get('https://us.puma.com/us/en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c0937a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "men_shoes=driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/main/div[1]/section/nav[1]/div/div/div/div[1]/div[1]/button')\n",
    "men_shoes.click()\n",
    "\n",
    "men_shoes1=driver.find_element(By.XPATH,'/html/body/div[9]/div/div/div[2]/div[1]')\n",
    "men_shoes1.click()\n",
    "\n",
    "men_shoes2=driver.find_element(By.XPATH,'//*[@id=\"tabregion-productdivName\"]/div/div/div/label[1]/div[1]')\n",
    "men_shoes2.click()\n",
    "\n",
    "show=driver.find_element(By.XPATH,'/html/body/div[9]/div/div/div[3]/button[2]/div')\n",
    "show.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b60ca6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ShoeName1=[]   \n",
    "Category1=[]  \n",
    "no_of_colors1=[]  \n",
    "Price1=[] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "54b981fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scroll down the page a bit,\n",
    "\n",
    "for _ in range(200):\n",
    "    driver.execute_script('window.scrollBy(0,4000)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a18d5a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets scrap the URL of all the shoes \n",
    "\n",
    "Product_url1=[]\n",
    "    \n",
    "url=driver.find_elements(By.XPATH,'//a[@class=\"tw-hqslau tw-xbcb1y\"]')\n",
    "for i in url:\n",
    "    Product_url1.append(i.get_attribute(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2474c70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_url1=Product_url1[1:151]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f7485017",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets Scrap for the first 150 Shoes for Table1 details\n",
    "\n",
    "#Name of the Shoe\n",
    "\n",
    "shoename=[]\n",
    "name=driver.find_elements(By.XPATH,'//h3[@class=\"w-full mobile:text-sm mobile:pr-0 font-bold text-base pr-5 line-clamp-2\"]')\n",
    "for a in name:\n",
    "    name1=a.text\n",
    "    shoename.append(name1)\n",
    "    \n",
    "ShoeName1=shoename[:150]\n",
    "\n",
    "\n",
    "#No of Colors of Shoe\n",
    "\n",
    "no_color=[]\n",
    "colo=driver.find_elements(By.XPATH,'//div[@class=\"relative h-12 mobile:h-10 flex items-center group\"]')\n",
    "for b in colo:\n",
    "    col=b.text\n",
    "    no_color.append(col)\n",
    "    \n",
    "no_of_colors1=no_color[:150]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#here most of the Shoes is refecting as basket ball, Golfshoes, running shoes and many more so however we are scrapping the data from men section we are giving the value as men\n",
    "\n",
    "Category1=['Men']*150\n",
    "\n",
    "len(Category1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Price of Shoes\n",
    "\n",
    "price1=[]\n",
    "Price=driver.find_elements(By.XPATH,'//span[@class=\"whitespace-nowrap text-base font-bold override:opacity-100\"]')\n",
    "for c in Price:\n",
    "    pr=c.text.replace('$','')   #we are not considered the $ in price, because we are keeping all the values in #dollar\n",
    "    price1.append(pr)\n",
    "    \n",
    "    \n",
    "Price1=price1[:150]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9d9d804a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets scrap the data which is inside of URl     \n",
    "Count_of_sizes_Men=[]\n",
    "code_Men=[]\n",
    "COLOR1=[]\n",
    "\n",
    "for url in product_url1:\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        size=driver.find_element(By.XPATH,'//*[@id=\"size-picker\"]')\n",
    "        Count_of_sizes_Men.append(size.text.count('Size\\n'))\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Count_of_sizes_Men.append(np.nan)\n",
    "        \n",
    "\n",
    "#ProductCode----------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "    try:\n",
    "        code=driver.find_element(By.XPATH,'//ul[@class=\"tw-1h4nwdw tw-p9uz4a tw-xwzea6 list-disc list-inside\"]/li[1]')\n",
    "        code_Men.append(code.text.split(':')[1])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        code_Men.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#Color1------------------------------------------------------------------------------------------\n",
    "\n",
    "    try:\n",
    "        COLOR=driver.find_element(By.XPATH,'//ul[@class=\"tw-1h4nwdw tw-p9uz4a tw-xwzea6 list-disc list-inside\"]/li[2]')\n",
    "        COLOR1.append(COLOR.text.split(':')[1])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        COLOR1.append(np.nan)\n",
    "        \n",
    "        \n",
    "##Color2-------------------------------------------------------------------------------------#\n",
    "\n",
    "    try:\n",
    "        button=driver.find_element(By.XPATH,'//*[@id=\"style-picker\"]/label[2]/input')\n",
    "        button.click()\n",
    "        col2=driver.find_element(By.XPATH,'//ul[@class=\"tw-1h4nwdw tw-p9uz4a tw-xwzea6 list-disc list-inside\"]/li[2]')\n",
    "        COLOR2.append(col2.text.split(':')[1].split('-')[0])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        COLOR2.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "##Color3-------------------------------------------------------------------------------------#\n",
    "    try:\n",
    "        button1=driver.find_element(By.XPATH,'//*[@id=\"style-picker\"]/label[3]/input')\n",
    "        button1.click()\n",
    "        col3=driver.find_element(By.XPATH,'//ul[@class=\"tw-1h4nwdw tw-p9uz4a tw-xwzea6 list-disc list-inside\"]/li[2]')\n",
    "        COLOR3.append(col3.text.split(':')[1].split('-')[0])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        COLOR3.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "##Color4---------------------------------------------------------------------------------------#\n",
    "    try:\n",
    "        button2=driver.find_element(By.XPATH,'//*[@id=\"style-picker\"]/label[4]/input')\n",
    "        button2.click()\n",
    "        col4=driver.find_element(By.XPATH,'//ul[@class=\"tw-1h4nwdw tw-p9uz4a tw-xwzea6 list-disc list-inside\"]/li[2]')\n",
    "        COLOR4.append(col4.text.split(':')[1].split('-')[0])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        COLOR4.append(np.nan)\n",
    "        \n",
    "        \n",
    "\n",
    "##Color5---------------------------------------------------------------------------------------#\n",
    "        \n",
    "    try:\n",
    "        button3=driver.find_element(By.XPATH,'//*[@id=\"style-picker\"]/label[5]/input')\n",
    "        button3.click()\n",
    "        col5=driver.find_element(By.XPATH,'//ul[@class=\"tw-1h4nwdw tw-p9uz4a tw-xwzea6 list-disc list-inside\"]/li[2]')\n",
    "        COLOR5.append(col5.text.split(':')[1].split('-')[0])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        COLOR5.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6b5b2bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MEN Table3\n",
    "\n",
    "Review1=[]\n",
    "Size1=[]\n",
    "Star1=[]\n",
    "Quality1=[]\n",
    "\n",
    "for url in product_url1:   \n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        rev=driver.find_element(By.XPATH,'//div[@class=\"tw-jgevef tw-1ewquor\"]/div/h2')\n",
    "        Review1.append(rev.text.split(' ')[1].split('(')[1].split(')')[0])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Review1.append(np.nan)\n",
    "        \n",
    "#Size available---------------       \n",
    "        \n",
    "    try:\n",
    "        size1=driver.find_element(By.XPATH,'//*[@id=\"size-picker\"]')\n",
    "        Size1.append(size1.text.replace('Size\\n',','))\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Size1.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#Star rating----------------------------\n",
    "\n",
    "\n",
    "    try:\n",
    "        star=driver.find_element(By.XPATH,'//div[@class=\"tw-jgevef tw-1ewquor\"]/div/span')\n",
    "        Star1.append(star.text.split(':')[1].split('/')[0])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Star1.append(np.nan)\n",
    "        \n",
    "        \n",
    "#Quality-------------------------------------------\n",
    "    try:\n",
    "        button=driver.find_element(By.XPATH,'//*[@id=\"product-reviews\"]/div[2]/button[2]')\n",
    "        button.click()\n",
    "        qu=driver.find_element(By.XPATH,'//p[@class=\"font-bold text-base\"]')\n",
    "        Quality1.append(qu.text.split(' ')[0])\n",
    "        close=driver.find_element(By.XPATH,'//*[@id=\"modal\"]/div/div/article/div[1]/button')\n",
    "        close.click()\n",
    "    except NoSuchElementException:\n",
    "        Quality1.append(np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ebd885",
   "metadata": {},
   "source": [
    "**Lets Scrape the details of Women's Shoe:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "25478ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrappig the details of Women\n",
    "\n",
    "Women=driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/div[1]/nav/div/ul/li[2]/div/div[1]/a/span')\n",
    "Women.click()\n",
    "\n",
    "women_shoes=driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/main/div[1]/section/nav[1]/div/div/div/div[1]/div[1]/button')\n",
    "women_shoes.click()\n",
    "\n",
    "women_shoes1=driver.find_element(By.XPATH,'//*[@id=\"heading-productdivName\"]')\n",
    "women_shoes1.click()\n",
    "\n",
    "women_shoes2=driver.find_element(By.XPATH,'//*[@id=\"tabregion-productdivName\"]/div/div/div/label[3]/div[1]')\n",
    "women_shoes2.click()\n",
    "\n",
    "show=driver.find_element(By.XPATH,'//*[@id=\"modal\"]/div/div[3]/button[2]/div')\n",
    "show.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "29530fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets scrap the URL of all the shoes \n",
    "\n",
    "Product_url101=[]\n",
    "    \n",
    "url=driver.find_elements(By.XPATH,'//a[@class=\"tw-hqslau tw-xbcb1y\"]')\n",
    "for i in url:\n",
    "    Product_url101.append(i.get_attribute(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "45c8978c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Product_url101=Product_url101[1:151]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cb788fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets Scrap for the first 150 Shoes for Table1 details\n",
    "\n",
    "#Name of the Shoe\n",
    "\n",
    "shoename1=[]\n",
    "name=driver.find_elements(By.XPATH,'//h3[@class=\"w-full mobile:text-sm mobile:pr-0 font-bold text-base pr-5 line-clamp-2\"]')\n",
    "for a in name:\n",
    "    name1=a.text\n",
    "    shoename1.append(name1)\n",
    "    \n",
    "ShoeName2=shoename1[:150]\n",
    "\n",
    "\n",
    "#No of Colors of Shoe\n",
    "\n",
    "no_color1=[]\n",
    "colo=driver.find_elements(By.XPATH,'//div[@class=\"relative h-12 mobile:h-10 flex items-center group\"]')\n",
    "for b in colo:\n",
    "    col=b.text\n",
    "    no_color1.append(col)\n",
    "    \n",
    "no_of_colors2=no_color1[:150]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#here most of the Shoes is refecting as basket ball, Golfshoes, running shoes and many more so however we are scrapping the data from women section we are giving the value as women\n",
    "\n",
    "Category2=['Women']*150\n",
    "\n",
    "len(Category2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Price of Shoes\n",
    "\n",
    "price2=[]\n",
    "Price=driver.find_elements(By.XPATH,'//div[@class=\"flex flex-col flex-none mobile:items-start items-end text-sm md:text-base mobile:mt-2\"]')\n",
    "for c in Price:\n",
    "    pr=c.text.replace('$','')   #we are not considered the $ in price, because we are keeping all the values in #dollar\n",
    "    price2.append(pr)\n",
    "    \n",
    "    \n",
    "Price2=price2[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ffce5e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets scrap the data which is inside of URl     \n",
    "Count_of_sizes_women=[]\n",
    "code_women=[]\n",
    "COLOR11=[]\n",
    "COLOR22=[]\n",
    "COLOR33=[]\n",
    "COLOR44=[]\n",
    "COLOR55=[]\n",
    "Review11=[]\n",
    "Size11=[]\n",
    "Star11=[]\n",
    "Quality11=[]\n",
    "\n",
    "for i in Product_url101:\n",
    "    driver.get(i)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        size=driver.find_element(By.XPATH,'//*[@id=\"size-picker\"]')\n",
    "        Count_of_sizes_women.append(size.text.count('Size\\n'))\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Count_of_sizes_women.append(np.nan)\n",
    "        \n",
    "\n",
    "#ProductCode----------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "    try:\n",
    "        code=driver.find_element(By.XPATH,'//ul[@class=\"tw-1h4nwdw tw-p9uz4a tw-xwzea6 list-disc list-inside\"]/li[1]')\n",
    "        code_women.append(code.text.split(':')[1])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        code_women.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#Color1------------------------------------------------------------------------------------------\n",
    "\n",
    "    try:\n",
    "        COLOR=driver.find_element(By.XPATH,'//ul[@class=\"tw-1h4nwdw tw-p9uz4a tw-xwzea6 list-disc list-inside\"]/li[2]')\n",
    "        COLOR11.append(COLOR.text.split(':')[1])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        COLOR11.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "##Color2-------------------------------------------------------------------------------------#\n",
    "\n",
    "    try:\n",
    "        button=driver.find_element(By.XPATH,'//*[@id=\"style-picker\"]/label[2]/input')\n",
    "        button.click()\n",
    "        col2=driver.find_element(By.XPATH,'//ul[@class=\"tw-1h4nwdw tw-p9uz4a tw-xwzea6 list-disc list-inside\"]/li[2]')\n",
    "        COLOR22.append(col2.text.split(':')[1].split('-')[0])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        COLOR22.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "##Color3-------------------------------------------------------------------------------------#\n",
    "    try:\n",
    "        button1=driver.find_element(By.XPATH,'//*[@id=\"style-picker\"]/label[3]/input')\n",
    "        button1.click()\n",
    "        col3=driver.find_element(By.XPATH,'//ul[@class=\"tw-1h4nwdw tw-p9uz4a tw-xwzea6 list-disc list-inside\"]/li[2]')\n",
    "        COLOR33.append(col3.text.split(':')[1].split('-')[0])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        COLOR33.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "##Color4---------------------------------------------------------------------------------------#\n",
    "    try:\n",
    "        button2=driver.find_element(By.XPATH,'//*[@id=\"style-picker\"]/label[4]/input')\n",
    "        button2.click()\n",
    "        col4=driver.find_element(By.XPATH,'//ul[@class=\"tw-1h4nwdw tw-p9uz4a tw-xwzea6 list-disc list-inside\"]/li[2]')\n",
    "        COLOR44.append(col4.text.split(':')[1].split('-')[0])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        COLOR44.append(np.nan)\n",
    "        \n",
    "        \n",
    "\n",
    "##Color5---------------------------------------------------------------------------------------#\n",
    "        \n",
    "    try:\n",
    "        button3=driver.find_element(By.XPATH,'//*[@id=\"style-picker\"]/label[5]/input')\n",
    "        button3.click()\n",
    "        col5=driver.find_element(By.XPATH,'//ul[@class=\"tw-1h4nwdw tw-p9uz4a tw-xwzea6 list-disc list-inside\"]/li[2]')\n",
    "        COLOR55.append(col5.text.split(':')[1].split('-')[0])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        COLOR55.append(np.nan)\n",
    "        \n",
    "    \n",
    "\n",
    "#Reviews--------------------------------------\n",
    "    \n",
    "    try:\n",
    "        rev=driver.find_element(By.XPATH,'//div[@class=\"tw-jgevef tw-1ewquor\"]/div/h2')\n",
    "        Review11.append(rev.text.split(' ')[1].split('(')[1].split(')')[0])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Review11.append(np.nan)\n",
    "        \n",
    "#Size available---------------       \n",
    "        \n",
    "    try:\n",
    "        size1=driver.find_element(By.XPATH,'//*[@id=\"size-picker\"]')\n",
    "        Size11.append(size1.text.replace('Size\\n',','))\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Size11.append(np.nan)\n",
    "        \n",
    "        \n",
    "#Star rating----------------------------\n",
    "\n",
    "\n",
    "    try:\n",
    "        star=driver.find_element(By.XPATH,'//div[@class=\"tw-jgevef tw-1ewquor\"]/div/span')\n",
    "        Star11.append(star.text.split(':')[1].split('/')[0])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Star11.append(np.nan)\n",
    "        \n",
    "        \n",
    "#Quality-------------------------------------------\n",
    "    try:\n",
    "        button=driver.find_element(By.XPATH,'//*[@id=\"product-reviews\"]/div[2]/button[2]')\n",
    "        button.click()\n",
    "        qu=driver.find_element(By.XPATH,'//p[@class=\"font-bold text-base\"]')\n",
    "        Quality11.append(qu.text.split(' ')[0])\n",
    "        close=driver.find_element(By.XPATH,'//*[@id=\"modal\"]/div/div/article/div[1]/button')\n",
    "        close.click()\n",
    "    except NoSuchElementException:\n",
    "        Quality11.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "fc581924",
   "metadata": {},
   "outputs": [],
   "source": [
    "ShoeName=ShoeName1+ShoeName2   \n",
    "Category=Category1+Category2\n",
    "no_of_colors=no_of_colors1+no_of_colors2\n",
    "Price=Price1+Price2\n",
    "cs=Count_of_sizes_Men+Count_of_sizes_women\n",
    "cc=code_Men+code_women\n",
    "c1=COLOR1+COLOR11\n",
    "c2=COLOR2+COLOR22\n",
    "c3=COLOR3+COLOR33\n",
    "c4=COLOR4+COLOR44\n",
    "c5=COLOR5+COLOR55\n",
    "r=Review1+Review11\n",
    "si=Size1+Size11\n",
    "q=Quality1+Quality11\n",
    "q=Quality1+Quality11\n",
    "st=Star1+Star11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "bb4f9f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dataframe for table1.\n",
    "df1=pd.DataFrame({'ShoeName':ShoeName,'Category':Category,'no_of_colors':no_of_colors,'Price':Price})\n",
    "\n",
    "#Saving into csv file.\n",
    "df1.to_csv('Puma_table1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0c33b356",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dataframe for table2\n",
    "df2=pd.DataFrame({'count_of_size':cs,'Color1':c1,'Color2':c2,'Color3':c3,'Color4':c4,'Color5':c5,'Product_code':cc})\n",
    "\n",
    "#Saving into csv file.\n",
    "df2.to_csv('Puma_table2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43107924",
   "metadata": {},
   "source": [
    "**In this specific brand, there appears to be a close association between comfort and quality, prompting me to consolidate them into a single category and populate the respective columns accordingly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "440c07ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dataframe for table3\n",
    "df3=pd.DataFrame({'reviews':r, 'Size':si, 'comfort':q, 'quality':q, 'star_rating':st})\n",
    "\n",
    "#Saving into csv file.\n",
    "df3.to_csv('Puma_table3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f4bdd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87935a1b",
   "metadata": {},
   "source": [
    "# 5.Nike"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399ad9c1",
   "metadata": {},
   "source": [
    "**First i will divide the Scrapping into two types, 1.Men and 2.Women**\n",
    "\n",
    "#First i will scrap the details of men Shoes\n",
    "\n",
    "#Creating a empty list and at last we will append the scrapped values into it.\n",
    "\n",
    "#Scrapping of 150 shoes for men and Women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e7faddcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome()     #United States\n",
    "driver.get('https://www.nike.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f01933d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First will Scrap the Mens Shoes\n",
    "\n",
    "Men=driver.find_element(By.XPATH,'/html/body/div[3]/div/div[3]/header/div[1]/div[2]/nav/div[2]/div/ul/li[2]/a')\n",
    "Men.click()\n",
    "\n",
    "\n",
    "men_shoes=driver.find_element(By.XPATH,'/html/body/div[4]/div/div/div[2]/div[2]/div/div/div/div/nav/div[1]/ul/li[1]/a')\n",
    "men_shoes.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "cf5f8306",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scroll down the page a bit,\n",
    "\n",
    "for _ in range(200):\n",
    "    driver.execute_script('window.scrollBy(0,5000)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "fde425e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets scrap the URL of all the shoes \n",
    "\n",
    "Product_url=[]\n",
    "    \n",
    "url=driver.find_elements(By.XPATH,'//a[@class=\"product-card__link-overlay\"]')\n",
    "for i in url:\n",
    "    Product_url.append(i.get_attribute(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "61db4e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reducing the list length to 150\n",
    "\n",
    "product_url=Product_url[:150]\n",
    "len(product_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3ac49256",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets Scrap for the first 150 Shoes for Table1 details\n",
    "\n",
    "#Name of the Shoe\n",
    "\n",
    "shoename=[]\n",
    "name=driver.find_elements(By.XPATH,'//a[@class=\"product-card__link-overlay\"]')\n",
    "for a in name:\n",
    "    name1=a.text\n",
    "    shoename.append(name1)\n",
    "    \n",
    "ShoeName1=shoename[:150]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#No of Colors of Shoe\n",
    "\n",
    "no_color=[]\n",
    "colo=driver.find_elements(By.XPATH,'//div[@class=\"product-card product-grid__card  css-1t0asop\"]/div/figure/div/div[2]/div/button/div')\n",
    "for b in colo:\n",
    "    col=b.text\n",
    "    no_color.append(col)\n",
    "    \n",
    "no_of_colors1=no_color[:150]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#here most of the Shoes is refecting as basket ball, Golfshoes, running shoes and many more so however we are scrapping the data from men section we are giving the value as men\n",
    "\n",
    "Category1=['Men']*150\n",
    "\n",
    "len(Category1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Price of Shoes\n",
    "\n",
    "price1=[]\n",
    "Price=driver.find_elements(By.XPATH,'//div[@class=\"product-card product-grid__card  css-1t0asop\"]/div/figure/div/div[3]/div/div/div/div')\n",
    "for c in Price:\n",
    "    pr=c.text.replace('$','')   #we are not considered the $ in price, because we are keeping all the values in #dollar\n",
    "    price1.append(pr)\n",
    "    \n",
    "    \n",
    "Price1=price1[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "82ee31e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets scrap the data which is inside of URl     \n",
    "Count_of_sizes11=[]\n",
    "code11=[]\n",
    "Star1=[]\n",
    "Quality1=[]\n",
    "Comfort1=[]\n",
    "\n",
    "for url in product_url:\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        size=driver.find_element(By.XPATH,'//*[@id=\"buyTools\"]/div[1]/fieldset/div')\n",
    "        Count_of_sizes11.append(size.text.count('\\n'))\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Count_of_sizes11.append(np.nan)\n",
    "        \n",
    "\n",
    "#ProductCode----------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "    try:\n",
    "        code=driver.find_element(By.XPATH,'//div[@class=\"pt6-sm prl6-sm prl0-lg\"]/div/ul/li[2]')\n",
    "        code11.append(code.text.split(':')[1])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        code11.append(np.nan)\n",
    "        \n",
    "\n",
    "##Color1-------------------------------------------------------------------------------------# \n",
    "    \n",
    "    try:\n",
    "        col1=driver.find_element(By.XPATH,'//div[@class=\"description-preview body-2 css-1pbvugb\"]/ul/li')\n",
    "        color1.append(col1.text.split(':')[1].split('/')[0])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color1.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "##Color2-------------------------------------------------------------------------------------#\n",
    "\n",
    "    try:\n",
    "        button=driver.find_element(By.XPATH,'//*[@id=\"ColorwayDiv\"]/div/div/fieldset/div[2]')\n",
    "        button.click()\n",
    "        col2=driver.find_element(By.XPATH,'//div[@class=\"description-preview body-2 css-1pbvugb\"]/ul/li')\n",
    "        color2.append(col2.text.split(':')[1].split('/')[0])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color2.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "##Color3-------------------------------------------------------------------------------------#\n",
    "    try:\n",
    "        button1=driver.find_element(By.XPATH,'//*[@id=\"ColorwayDiv\"]/div/div/fieldset/div[3]')\n",
    "        button1.click()\n",
    "        col3=driver.find_element(By.XPATH,'//div[@class=\"description-preview body-2 css-1pbvugb\"]/ul/li')\n",
    "        color3.append(col3.text.split(':')[1].split('/')[0])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color3.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "##Color4---------------------------------------------------------------------------------------#\n",
    "    try:\n",
    "        button2=driver.find_element(By.XPATH,'//*[@id=\"ColorwayDiv\"]/div/div/fieldset/div[4]')\n",
    "        button2.click()\n",
    "        col4=driver.find_element(By.XPATH,'//div[@class=\"description-preview body-2 css-1pbvugb\"]/ul/li')\n",
    "        color4.append(col4.text.split(':')[1].split('/')[0])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color4.append(np.nan)\n",
    "        \n",
    "        \n",
    "\n",
    "##Color5---------------------------------------------------------------------------------------#\n",
    "        \n",
    "    try:\n",
    "        button3=driver.find_element(By.XPATH,'//*[@id=\"ColorwayDiv\"]/div/div/fieldset/div[5]')\n",
    "        button3.click()\n",
    "        col5=driver.find_element(By.XPATH,'//div[@class=\"description-preview body-2 css-1pbvugb\"]/ul/li')\n",
    "        color5.append(col5.text.split(':')[1].split('/')[0])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color5.append(np.nan)   \n",
    "        \n",
    "        \n",
    "##Reviews-----------\n",
    "\n",
    "    try:\n",
    "        rev=driver.find_element(By.XPATH,'//div[@class=\"css-w11ebw\"]/details[2]/summary/h3/span')\n",
    "        reviews1.append(rev.text.split(' ')[1].split('(')[1].split(')')[0])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        reviews1.append(np.nan)\n",
    "        \n",
    "#Size available---------------       \n",
    "        \n",
    "    try:\n",
    "        size1=driver.find_element(By.XPATH,'//*[@id=\"buyTools\"]/div[1]/fieldset/div')\n",
    "        Size1.append(size1.text.replace('\\n',','))\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Size1.append(np.nan)\n",
    "        \n",
    "        \n",
    "#Star rating----------------------------\n",
    "    try:\n",
    "        star=driver.find_element(By.XPATH,'//div[@class=\"css-n209rx\"]')\n",
    "        Star1.append(star.get_attribute(\"aria-label\").strip())\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Star1.append(np.nan)\n",
    "        \n",
    "        \n",
    "#Quality-------------------------------------------\n",
    "    try:\n",
    "        button1=driver.find_element(By.XPATH,'//*[@id=\"RightRail\"]/div[2]/div[4]/div/details[2]/div/div/div/p/button')\n",
    "        button1.click()\n",
    "        qu=driver.find_element(By.XPATH,'//div[@class=\"tt-l-grid tt-c-reviews-summary__content\"]/div[2]/div[2]/div/div[2]/div[2]')\n",
    "        Quality1.append(qu.text.split(' ')[0])\n",
    "        close=driver.find_element(By.XPATH,'//*[@id=\"PDP\"]/div[2]/div/div[4]/div[6]/div[1]/button[2]')\n",
    "        close.click()\n",
    "    except NoSuchElementException:\n",
    "        Quality1.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "##Comfort-------------------------------------------\n",
    "    try:\n",
    "        button=driver.find_element(By.XPATH,'//*[@id=\"RightRail\"]/div[2]/div[3]/div/details[2]')\n",
    "        button.click()\n",
    "        button1=driver.find_element(By.XPATH,'//*[@id=\"RightRail\"]/div[2]/div[4]/div/details[2]/div/div/div/p/button')\n",
    "        button1.click()\n",
    "        co=driver.find_element(By.XPATH,'//div[@class=\"tt-l-grid tt-c-reviews-summary__content\"]/div[2]/div[2]/div/div[2]/div[2]')\n",
    "        Comfort1.append(co.text.split(' ')[0])\n",
    "        close=driver.find_element(By.XPATH,'//*[@id=\"PDP\"]/div[2]/div/div[4]/div[6]/div[1]/button[2]')\n",
    "        close.click()\n",
    "    except NoSuchElementException:\n",
    "        Comfort1.append(np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d7685b",
   "metadata": {},
   "source": [
    "**Lets Scrape the details of Women's Shoe:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "87255542",
   "metadata": {},
   "outputs": [],
   "source": [
    "ShoeName2=[]   \n",
    "Category2=[]  \n",
    "no_of_colors2=[]  \n",
    "Price2=[] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "7876cf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrappig the details of Women\n",
    "\n",
    "Women=driver.find_element(By.XPATH,'//*[@id=\"gen-nav-commerce-header-v2\"]/div[3]/header/div[1]/div[2]/nav/div[2]/div/ul/li[3]/a')\n",
    "Women.click()\n",
    "\n",
    "women_shoes=driver.find_element(By.XPATH,'/html/body/div[4]/div/div/div[2]/div[2]/div/div/div/div/nav/div[1]/ul/li[1]')\n",
    "women_shoes.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d5120e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scroll down the page a bit,\n",
    "\n",
    "for _ in range(200):\n",
    "    driver.execute_script('window.scrollBy(0,5000)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "7d708132",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets scrap the URL of all the shoes \n",
    "\n",
    "Product_url1=[]\n",
    "    \n",
    "url=driver.find_elements(By.XPATH,'//a[@class=\"product-card__link-overlay\"]')\n",
    "for i in url:\n",
    "    Product_url1.append(i.get_attribute(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "6be92b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_url1=Product_url1[:150]\n",
    "len(product_url1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "e8b7279a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets Scrap for the first 150 Shoes for Table1 details for women\n",
    "\n",
    "#Name of the Shoe\n",
    "\n",
    "shoename1=[]\n",
    "name=driver.find_elements(By.XPATH,'//a[@class=\"product-card__link-overlay\"]')\n",
    "for a in name:\n",
    "    name1=a.text\n",
    "    shoename1.append(name1)\n",
    "    \n",
    "ShoeName2=shoename1[:150]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#No of Colors of Shoe\n",
    "\n",
    "no_color1=[]\n",
    "colo=driver.find_elements(By.XPATH,'//div[@class=\"product-card product-grid__card  css-1t0asop\"]/div/figure/div/div[2]/div/button/div')\n",
    "for b in colo:\n",
    "    col=b.text\n",
    "    no_color1.append(col)\n",
    "    \n",
    "no_of_colors2=no_color1[:150]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#here most of the Shoes is refecting as basket ball, Golfshoes, running shoes and many more so however we are scrapping the data from men section we are giving the value as men\n",
    "\n",
    "Category2=['Women']*150\n",
    "\n",
    "len(Category2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Price of Shoes\n",
    "\n",
    "price2=[]\n",
    "Price=driver.find_elements(By.XPATH,'//div[@class=\"product-card product-grid__card  css-1t0asop\"]/div/figure/div/div[3]/div/div/div/div')\n",
    "for c in Price:\n",
    "    pr=c.text.replace('$','')   #we are not considered the $ in price, because we are keeping all the values in #dollar\n",
    "    price2.append(pr)\n",
    "    \n",
    "    \n",
    "Price2=price2[:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "10af6530",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets scrap the data which is inside of URl     \n",
    "Count_of_sizes1=[]\n",
    "code1=[]\n",
    "color11=[]\n",
    "color22=[]\n",
    "color33=[]\n",
    "color44=[]\n",
    "color55=[]\n",
    "Star11=[]\n",
    "Quality11=[]\n",
    "Comfort11=[]\n",
    "\n",
    "for url in product_url1:\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    try:\n",
    "        size=driver.find_element(By.XPATH,'//*[@id=\"buyTools\"]/div[1]/fieldset/div')\n",
    "        Count_of_sizes1.append(size.text.count('\\n'))\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Count_of_sizes1.append(np.nan)\n",
    "        \n",
    "\n",
    "#ProductCode----------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "    try:\n",
    "        code=driver.find_element(By.XPATH,'//div[@class=\"pt6-sm prl6-sm prl0-lg\"]/div/ul/li[2]')\n",
    "        code1.append(code.text.split(':')[1])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        code1.append(np.nan)\n",
    "        \n",
    "\n",
    "\n",
    "##Color1-------------------------------------------------------------------------------------#   \n",
    "    \n",
    "    try:\n",
    "        col1=driver.find_element(By.XPATH,'//div[@class=\"description-preview body-2 css-1pbvugb\"]/ul/li')\n",
    "        color11.append(col1.text.split(':')[1].split('/')[0])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color11.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "##Color2-------------------------------------------------------------------------------------#\n",
    "\n",
    "    try:\n",
    "        button=driver.find_element(By.XPATH,'//*[@id=\"ColorwayDiv\"]/div/div/fieldset/div[2]')\n",
    "        button.click()\n",
    "        col2=driver.find_element(By.XPATH,'//div[@class=\"description-preview body-2 css-1pbvugb\"]/ul/li')\n",
    "        color22.append(col2.text.split(':')[1].split('/')[0])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color22.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "##Color3-------------------------------------------------------------------------------------#\n",
    "    try:\n",
    "        button1=driver.find_element(By.XPATH,'//*[@id=\"ColorwayDiv\"]/div/div/fieldset/div[3]')\n",
    "        button1.click()\n",
    "        col3=driver.find_element(By.XPATH,'//div[@class=\"description-preview body-2 css-1pbvugb\"]/ul/li')\n",
    "        color33.append(col3.text.split(':')[1].split('/')[0])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color33.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "##Color4---------------------------------------------------------------------------------------#\n",
    "    try:\n",
    "        button2=driver.find_element(By.XPATH,'//*[@id=\"ColorwayDiv\"]/div/div/fieldset/div[4]')\n",
    "        button2.click()\n",
    "        col4=driver.find_element(By.XPATH,'//div[@class=\"description-preview body-2 css-1pbvugb\"]/ul/li')\n",
    "        color44.append(col4.text.split(':')[1].split('/')[0])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color44.append(np.nan)\n",
    "        \n",
    "        \n",
    "\n",
    "##Color5---------------------------------------------------------------------------------------#\n",
    "        \n",
    "    try:\n",
    "        button3=driver.find_element(By.XPATH,'//*[@id=\"ColorwayDiv\"]/div/div/fieldset/div[5]')\n",
    "        button3.click()\n",
    "        col5=driver.find_element(By.XPATH,'//div[@class=\"description-preview body-2 css-1pbvugb\"]/ul/li')\n",
    "        color55.append(col5.text.split(':')[1].split('/')[0])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color55.append(np.nan) \n",
    "        \n",
    "        \n",
    "#Reviews---------------------------------------------------------\n",
    "\n",
    "    \n",
    "    try:\n",
    "        rev=driver.find_element(By.XPATH,'//div[@class=\"css-w11ebw\"]/details[2]/summary/h3/span')\n",
    "        reviews2.append(rev.text.split(' ')[1].split('(')[1].split(')')[0])\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        reviews2.append(np.nan)\n",
    "        \n",
    "#Size available---------------       \n",
    "        \n",
    "    try:\n",
    "        size1=driver.find_element(By.XPATH,'//*[@id=\"buyTools\"]/div[1]/fieldset/div')\n",
    "        Size2.append(size1.text.replace('\\n',','))\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Size2.append(np.nan)\n",
    "        \n",
    "        \n",
    "#Star rating----------------------------\n",
    "    try:\n",
    "        star=driver.find_element(By.XPATH,'//div[@class=\"css-n209rx\"]')\n",
    "        Star11.append(star.get_attribute(\"aria-label\").strip())\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Star11.append(np.nan)\n",
    "        \n",
    "        \n",
    "#Quality-------------------------------------------\n",
    "    try:\n",
    "        button1=driver.find_element(By.XPATH,'//*[@id=\"RightRail\"]/div[2]/div[4]/div/details[2]/div/div/div/p/button')\n",
    "        button1.click()\n",
    "        qu=driver.find_element(By.XPATH,'//div[@class=\"tt-l-grid tt-c-reviews-summary__content\"]/div[2]/div[2]/div/div[2]/div[2]')\n",
    "        Quality11.append(qu.text.split(' ')[0])\n",
    "        close=driver.find_element(By.XPATH,'//*[@id=\"PDP\"]/div[2]/div/div[4]/div[6]/div[1]/button[2]')\n",
    "        close.click()\n",
    "    except NoSuchElementException:\n",
    "        Quality11.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "#Comfort-------------------------------------------\n",
    "    try:\n",
    "        button=driver.find_element(By.XPATH,'//*[@id=\"RightRail\"]/div[2]/div[3]/div/details[2]')\n",
    "        button.click()\n",
    "        button1=driver.find_element(By.XPATH,'//*[@id=\"RightRail\"]/div[2]/div[4]/div/details[2]/div/div/div/p/button')\n",
    "        button1.click()\n",
    "        co=driver.find_element(By.XPATH,'//div[@class=\"tt-l-grid tt-c-reviews-summary__content\"]/div[2]/div[2]/div/div[2]/div[2]')\n",
    "        Comfort11.append(co.text.split(' ')[0])\n",
    "        close=driver.find_element(By.XPATH,'//*[@id=\"PDP\"]/div[2]/div/div[4]/div[6]/div[1]/button[2]')\n",
    "        close.click()\n",
    "    except NoSuchElementException:\n",
    "        Comfort11.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "4ed1f8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ShoeName=ShoeName1+ShoeName2   \n",
    "Category=Category1+Category2\n",
    "no_of_colors=no_of_colors1+no_of_colors2\n",
    "Price=Price1+Price2\n",
    "count_of_size=Count_of_sizes11+Count_of_sizes1\n",
    "Color1=color1+color11\n",
    "Color2=color2+color22\n",
    "Color3=color3+color33\n",
    "Color4=color4+color33\n",
    "Color5=color5+color55\n",
    "Product_code=code11+code1\n",
    "Reviews=reviews1+reviews2\n",
    "Sizes=Size1+Size2\n",
    "st=Star1+Star11\n",
    "Com=Comfort1+Comfort11\n",
    "Qua=Quality1+Quality11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "c03826c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the table1\n",
    "df1=pd.DataFrame({'ShoeName':ShoeName,'Category':Category,'no_of_colors':no_of_colors,'Price':Price})\n",
    "\n",
    "#saving the file in csv:\n",
    "df1.to_csv('nike_table1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "444d5562",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the table2\n",
    "df2=pd.DataFrame({'count_of_size':count_of_size,'Color1':Color1,'Color2':Color2,'Color3':Color3,'Color4':Color4,'Color5':Color5,'Product_code':Product_code})\n",
    "\n",
    "\n",
    "#saving the file in csv:\n",
    "df2.to_csv('nike_table2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "8fff0ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the table3\n",
    "df3=pd.DataFrame({'reviews':Reviews,'Size':Sizes,'comfort':Com,'quality':Qua,'star_rating':st})\n",
    "\n",
    "#saving the file in csv:\n",
    "df3.to_csv('nike_table3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6400f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccef2207",
   "metadata": {},
   "source": [
    "# WoodLand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93451678",
   "metadata": {},
   "source": [
    "**First i will divide the Scrapping into two types, 1.Men and 2.Women**\n",
    "\n",
    "#First i will scrap the details of men Shoes\n",
    "\n",
    "#Creating a empty list and at last we will append the scrapped values into it.\n",
    "\n",
    "#Scrapping of 150 shoes for men and Women"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "569b17ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome()     \n",
    "driver.get('https://www.woodlandworldwide.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "b4d2b587",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets scrape the mens shoe details\n",
    "\n",
    "driver=webdriver.Chrome()     \n",
    "driver.get('https://www.woodlandworldwide.com/product-list/MEN_SNEAKERS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "bb28d3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scroll down the page a bit,\n",
    "\n",
    "for _ in range(150):\n",
    "    driver.execute_script('window.scrollBy(0,4000)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "f782c2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets scrap the URL of all the shoes #Only 111 Sneakers found lets scrape those.\n",
    "\n",
    "Product_url1=[]\n",
    "    \n",
    "url=driver.find_elements(By.XPATH,'//a[@class=\"MuiTypography-root MuiTypography-inherit MuiLink-root MuiLink-underlineAlways css-15kjgm\"]')\n",
    "for i in url:\n",
    "    Product_url1.append(i.get_attribute(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cb403c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ShoeName11=[] \n",
    "Price11=[]\n",
    "Count_of_sizes11=[]\n",
    "Reviews11=[]\n",
    "Size11=[]\n",
    "Star11=[]\n",
    "color11=[]\n",
    "color22=[]\n",
    "color33=[]\n",
    "color44=[]\n",
    "color55=[]\n",
    "Code_Men11=[]\n",
    "Quality11=[]\n",
    "Comfort11=[]\n",
    "no_of_colors11=[]\n",
    "Category11=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "1b34ae80",
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in Product_url1:\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "    \n",
    "#Shoename:\n",
    "    try:\n",
    "        name=driver.find_element(By.XPATH,'//h2[@class=\"productprice_productname__7Lzg5\"]')\n",
    "        ShoeName11.append(name.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        ShoeName11.append(np.nan)\n",
    "\n",
    "    \n",
    "    \n",
    "#Price:\n",
    "    try:\n",
    "        price=driver.find_element(By.XPATH,'//div[@class=\"productprice_productprice__IuYyr\"]/span[1]')\n",
    "        Price11.append(price.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Price11.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "#Count_of_size;         \n",
    "\n",
    "    try:\n",
    "        size=driver.find_element(By.XPATH,'//div[@class=\"MuiBox-root css-acwcvw\"]/div')\n",
    "        Count_of_sizes11.append(size.text.count('\\n'))\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Count_of_sizes11.append(np.nan)\n",
    "    \n",
    "    \n",
    "#Size available---------------       \n",
    "        \n",
    "    try:\n",
    "        size1=driver.find_element(By.XPATH,'//div[@class=\"MuiBox-root css-acwcvw\"]/div')\n",
    "        a=size1.text.replace('\\n',',')\n",
    "        Size11.append(a.replace('Left','').replace('Sold out',''))\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Size11.append(np.nan)\n",
    "\n",
    "\n",
    "#Color1----------------------------\n",
    "    try:\n",
    "        col1=driver.find_element(By.XPATH,'//h5[@class=\"colorpicker_colorname__sRdk8\"]')\n",
    "        color11.append(col1.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color11.append(np.nan)\n",
    "        \n",
    "        \n",
    "##Color2-------------------------------------------------------------------------------------#\n",
    "\n",
    "    try:\n",
    "        button=driver.find_element(By.XPATH,'//div[@class=\"MuiGrid-root MuiGrid-item MuiGrid-grid-xs-12 MuiGrid-grid-md-5 css-1r482s6\"]/div[2]/div/div[2]/div[2]')\n",
    "        button.click()\n",
    "        time.sleep(3)\n",
    "        col2=driver.find_element(By.XPATH,'//h5[@class=\"colorpicker_colorname__sRdk8\"]')\n",
    "        color22.append(col2.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color22.append(np.nan)\n",
    "        \n",
    "        \n",
    "##Color3-------------------------------------------------------------------------------------#\n",
    "\n",
    "    try:\n",
    "        button=driver.find_element(By.XPATH,'//div[@class=\"MuiGrid-root MuiGrid-item MuiGrid-grid-xs-12 MuiGrid-grid-md-5 css-1r482s6\"]/div[2]/div/div[2]/div[3]')\n",
    "        button.click()\n",
    "        time.sleep(3)\n",
    "        col3=driver.find_element(By.XPATH,'//h5[@class=\"colorpicker_colorname__sRdk8\"]')\n",
    "        color33.append(col3.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color33.append(np.nan)\n",
    "        \n",
    "        \n",
    "##Color4-------------------------------------------------------------------------------------#\n",
    "\n",
    "    try:\n",
    "        button=driver.find_element(By.XPATH,'//div[@class=\"MuiGrid-root MuiGrid-item MuiGrid-grid-xs-12 MuiGrid-grid-md-5 css-1r482s6\"]/div[2]/div/div[2]/div[4]')\n",
    "        button.click()\n",
    "        time.sleep(3)\n",
    "        col4=driver.find_element(By.XPATH,'//h5[@class=\"colorpicker_colorname__sRdk8\"]')\n",
    "        color44.append(col4.text)\n",
    "\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color44.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "##Color5-------------------------------------------------------------------------------------#\n",
    "\n",
    "    try:\n",
    "        button=driver.find_element(By.XPATH,'//div[@class=\"MuiGrid-root MuiGrid-item MuiGrid-grid-xs-12 MuiGrid-grid-md-5 css-1r482s6\"]/div[2]/div/div[2]/div[5]')\n",
    "        button.click()\n",
    "        time.sleep(3)\n",
    "        col5=driver.find_element(By.XPATH,'//h5[@class=\"colorpicker_colorname__sRdk8\"]')\n",
    "        color55.append(col5.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color55.append(np.nan)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941f5388",
   "metadata": {},
   "source": [
    "**Lets Scrape the details of Women's Shoe:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a348991b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets scrape the mens shoe details\n",
    "\n",
    "driver=webdriver.Chrome()     \n",
    "driver.get('https://www.woodlandworldwide.com/product-list/WOMEN_SNEAKERS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "21a0aa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scroll down the page a bit,\n",
    "\n",
    "for _ in range(150):\n",
    "    driver.execute_script('window.scrollBy(0,4000)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "61741aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets scrap the URL of all the shoes #Only 111 Sneakers found lets scrape those.\n",
    "\n",
    "Product_url2=[]\n",
    "    \n",
    "url=driver.find_elements(By.XPATH,'//a[@class=\"MuiTypography-root MuiTypography-inherit MuiLink-root MuiLink-underlineAlways css-15kjgm\"]')\n",
    "for i in url:\n",
    "    Product_url2.append(i.get_attribute(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "2f4f3df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ShoeName111=[] \n",
    "Price111=[]\n",
    "Count_of_sizes111=[]\n",
    "Reviews111=[]\n",
    "Size111=[]\n",
    "Star111=[]\n",
    "color111=[]\n",
    "color222=[]\n",
    "color333=[]\n",
    "color444=[]\n",
    "color555=[]\n",
    "Code_women111=[]\n",
    "Quality111=[]\n",
    "Comfort111=[]\n",
    "no_of_colors111=[]\n",
    "Category111=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "21730b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in Product_url2:\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "    \n",
    "#Shoename:\n",
    "    try:\n",
    "        name=driver.find_element(By.XPATH,'//h2[@class=\"productprice_productname__7Lzg5\"]')\n",
    "        ShoeName111.append(name.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        ShoeName111.append(np.nan)\n",
    "\n",
    "    \n",
    "    \n",
    "#Price:\n",
    "    try:\n",
    "        price=driver.find_element(By.XPATH,'//div[@class=\"productprice_productprice__IuYyr\"]/span[1]')\n",
    "        Price111.append(price.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Price111.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "#Count_of_size;         \n",
    "\n",
    "    try:\n",
    "        size=driver.find_element(By.XPATH,'//div[@class=\"MuiBox-root css-acwcvw\"]/div')\n",
    "        Count_of_sizes111.append(size.text.count('\\n'))\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Count_of_sizes111.append(np.nan)\n",
    "    \n",
    "    \n",
    "#Size available---------------       \n",
    "        \n",
    "    try:\n",
    "        size1=driver.find_element(By.XPATH,'//div[@class=\"MuiBox-root css-acwcvw\"]/div')\n",
    "        a=size1.text.replace('\\n',',')\n",
    "        Size111.append(a.replace('Left','').replace('Sold out',''))\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        Size111.append(np.nan)\n",
    "\n",
    "\n",
    "#Color1----------------------------\n",
    "    try:\n",
    "        col1=driver.find_element(By.XPATH,'//h5[@class=\"colorpicker_colorname__sRdk8\"]')\n",
    "        color111.append(col1.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color111.append(np.nan)\n",
    "        \n",
    "        \n",
    "##Color2-------------------------------------------------------------------------------------#\n",
    "\n",
    "    try:\n",
    "        button=driver.find_element(By.XPATH,'//div[@class=\"MuiGrid-root MuiGrid-item MuiGrid-grid-xs-12 MuiGrid-grid-md-5 css-1r482s6\"]/div[2]/div/div[2]/div[2]')\n",
    "        button.click()\n",
    "        time.sleep(3)\n",
    "        col2=driver.find_element(By.XPATH,'//h5[@class=\"colorpicker_colorname__sRdk8\"]')\n",
    "        color222.append(col2.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color222.append(np.nan)\n",
    "        \n",
    "        \n",
    "##Color3-------------------------------------------------------------------------------------#\n",
    "\n",
    "    try:\n",
    "        button=driver.find_element(By.XPATH,'//div[@class=\"MuiGrid-root MuiGrid-item MuiGrid-grid-xs-12 MuiGrid-grid-md-5 css-1r482s6\"]/div[2]/div/div[2]/div[3]')\n",
    "        button.click()\n",
    "        time.sleep(3)\n",
    "        col3=driver.find_element(By.XPATH,'//h5[@class=\"colorpicker_colorname__sRdk8\"]')\n",
    "        color333.append(col3.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color333.append(np.nan)\n",
    "        \n",
    "        \n",
    "##Color4-------------------------------------------------------------------------------------#\n",
    "\n",
    "    try:\n",
    "        button=driver.find_element(By.XPATH,'//div[@class=\"MuiGrid-root MuiGrid-item MuiGrid-grid-xs-12 MuiGrid-grid-md-5 css-1r482s6\"]/div[2]/div/div[2]/div[4]')\n",
    "        button.click()\n",
    "        time.sleep(3)\n",
    "        col4=driver.find_element(By.XPATH,'//h5[@class=\"colorpicker_colorname__sRdk8\"]')\n",
    "        color444.append(col4.text)\n",
    "\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color444.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "##Color5-------------------------------------------------------------------------------------#\n",
    "\n",
    "    try:\n",
    "        button=driver.find_element(By.XPATH,'//div[@class=\"MuiGrid-root MuiGrid-item MuiGrid-grid-xs-12 MuiGrid-grid-md-5 css-1r482s6\"]/div[2]/div/div[2]/div[5]')\n",
    "        button.click()\n",
    "        time.sleep(3)\n",
    "        col5=driver.find_element(By.XPATH,'//h5[@class=\"colorpicker_colorname__sRdk8\"]')\n",
    "        color555.append(col5.text)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        color555.append(np.nan)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "1e53a3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this dataset, the majority of shoes are categorized as basketball, golf shoes, running shoes, and various others. Consequently, during the data scraping process from the men's section, we are assigning the value 'men'and 'women' to appropriately represent this category.\n",
    "\n",
    "Category111=['Women']*77\n",
    "\n",
    "len(Category111)\n",
    "\n",
    "\n",
    "Category11=['Men']*111\n",
    "\n",
    "len(Category11)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba44f0f",
   "metadata": {},
   "source": [
    "**In the Woodland dataset, there are no available reviews, comfort rates, star ratings, quality rates, or product codes. Therefore, these fields have been assigned null values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "af5f5467",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making all values as NULL for men\n",
    "Star11=Reviews11=Code_Men11=Comfort11=Quality11=[np.nan]*111\n",
    "\n",
    "#Making all values as NULL for Women\n",
    "Star111=Reviews111=Code_women11=Comfort111=Quality111=[np.nan]*77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "b6c5c145",
   "metadata": {},
   "outputs": [],
   "source": [
    "sh=ShoeName11+ShoeName111\n",
    "pr=Price11+Price111\n",
    "co=Count_of_sizes11+Count_of_sizes111\n",
    "r=Reviews11+Reviews111\n",
    "si=Size11+Size111\n",
    "st=Star11+Star111\n",
    "c1=color11+color111\n",
    "c2=color22+color222\n",
    "c3=color33+color333\n",
    "c4=color44+color444\n",
    "c5=color55+color555\n",
    "qu=Quality11+Quality111\n",
    "com=Comfort11+Comfort111\n",
    "nc=no_of_colors11+no_of_colors111\n",
    "cd=Code_Men11+Code_women11\n",
    "cat=Category11+Category111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "826008d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Table No 1\n",
    "df1=pd.DataFrame({'ShoeName':sh,'Category':cat,'no_of_colors':nc,'Price':pr})\n",
    "\n",
    "#saving file in CSV\n",
    "df1.to_csv('woodland_table1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "82618bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Table No 2\n",
    "df2=pd.DataFrame({'count_of_size':co,'Color1':c1,'Color2':c2,'Color3':c3,'Color4':c4,'Color5':c5,'Product_code':cd})\n",
    "\n",
    "#saving file in CSV\n",
    "df2.to_csv('woodland_table2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "9a14d77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Table No 3\n",
    "df3=pd.DataFrame({'reviews':r, 'Size':si, 'comfort':com, 'quality':qu, 'star_rating':st})\n",
    "\n",
    "#saving file in CSV\n",
    "df3.to_csv('woodland_table3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795ab741",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e042077",
   "metadata": {},
   "source": [
    "**The data scraping process has been completed, and I am in the process of addressing a few minor corrections. I am making adjustments to the CSV file accordingly and preparing the three tables from the CSV data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8e9594",
   "metadata": {},
   "source": [
    "**We will convert the currency of the Woodland, Columbia, and Skechers datasets to USD. This is necessary because their data is scraped in Rupees, as the US website does not permit automation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f28c0823",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skechers TABLE NO 1\n",
    "df1=pd.read_csv('Skechers_table1.csv')\n",
    "\n",
    "#1 Indian Rupee = 0.012 United States Dollar\n",
    "exchange_rate = 0.012\n",
    "\n",
    "# Converting Rupees into Dollars($)\n",
    "df1['Price'] = df1['Price'] * exchange_rate\n",
    "df1['Price'] = df1['Price'].round(2)\n",
    "\n",
    "#Taking out colors from Number of Colors\n",
    "df1['no_of_colors']=df1['no_of_colors'].astype(str)\n",
    "df1['no_of_colors']=df1['no_of_colors'].str.replace('colors', '')\n",
    "df1\n",
    "\n",
    "#Lets save the dataset\n",
    "df1.to_csv('Skechers_table1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7297ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Columbia\n",
    "df7=pd.read_csv('Colombia_table1.csv')\n",
    "\n",
    "#Taking out colors from Number of Colors\n",
    "df7['no_of_colors']=df7['no_of_colors'].astype(str)\n",
    "df7['no_of_colors']=df7['no_of_colors'].str.replace('colors', '')\n",
    "df7\n",
    "\n",
    "df7['Price'] = df7['Price'] * exchange_rate\n",
    "df7['Price'] = df7['Price'].round(2)\n",
    "\n",
    "#saving the dataset\n",
    "df7.to_csv('Colombia_table1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7de7101c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Woodland\n",
    "\n",
    "#Taking out colors from Number of Colors\n",
    "df9['no_of_colors']=df9['no_of_colors'].astype(str)\n",
    "df9['no_of_colors']=df9['no_of_colors'].str.replace('colors', '')\n",
    "\n",
    "\n",
    "df9['Price'] = df9['Price'] * exchange_rate\n",
    "df9['Price'] = df9['Price'].round(2)\n",
    "\n",
    "#save the csv data\n",
    "df9.to_csv('woodland_table1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4291ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
